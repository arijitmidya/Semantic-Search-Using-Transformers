{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Semantic Search with Transformers\n",
    "\n",
    "In this project, you’ll use the sentence_transformers library to perform semantic search on a corpus of machine learning papers. The \n",
    "sentence_transformers library enables us to easily generate embeddings for any text using Transformer-based models. Semantic similarity can then be \n",
    "modeled as the distance between two embeddings.\n",
    "\n",
    "To complete this project, you’ll perform the following tasks:\n",
    "\n",
    "1. Generate embeddings for each paper summary.\n",
    "                      \n",
    "2. Create an index for efficient search using Facebook’s Faiss library.\n",
    "    \n",
    "3. Test the search engine using custom prompts and summaries from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUsc-D_dbJXf"
   },
   "source": [
    "## Task 1: Import the Libraries\n",
    "\n",
    "Let’s start this project by importing the modules required for completing this project.\n",
    "\n",
    "To complete this task, import the following libraries:\n",
    "\n",
    "1. pandas: This module is used for loading and displaying the dataset.\n",
    "                                                          \n",
    "2. torch: This module is used to create and manipulate document embedding matrices.\n",
    "                                                          \n",
    "3. SentenceTransformer: This is a method in the sentence_transformers library and is used for retrieving the Transformer-based model.\n",
    "                                                          \n",
    "4. preprocessing: This is a submodule of the sklearn package and will be used to preprocess the data.\n",
    "\n",
    "5. faiss: This library is used to create, store, and use the search index.\n",
    "\n",
    "6. numpy: This package provides numerical computing capabilities and is useful for various calculations.\n",
    "\n",
    "7. pickle: This module is used to load and store the model and embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fU2i4vlCVyRc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn import preprocessing\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miSbJ1I9bPa-"
   },
   "source": [
    "## Task 2: Load the Data\n",
    "\n",
    "First, let’s load the dataset as a pandas DataFrame. The dataset is made available as a file named arxivData.json in the ./usercode directory, \n",
    "and consists of the metadata of 41,000+ research papers.\n",
    "\n",
    "Perform the following steps to complete this task:\n",
    "\n",
    "1. Load the dataset to a pandas DataFrame.\n",
    "    \n",
    "2. Drop the author, link, and tag columns of the dataset.\n",
    "    \n",
    "3. Display the dataset header using the head() method.\n",
    "    \n",
    "4. Print the number of machine learning papers in the dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UqsNF7ZpXoS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Machine Learning papers:  41000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1802.00209v1</td>\n",
       "      <td>2</td>\n",
       "      <td>We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.</td>\n",
       "      <td>Dual Recurrent Attention Units for Visual Question Answering</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1603.03827v1</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.</td>\n",
       "      <td>Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1606.00776v2</td>\n",
       "      <td>6</td>\n",
       "      <td>We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.</td>\n",
       "      <td>Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>1705.08142v2</td>\n",
       "      <td>5</td>\n",
       "      <td>Multi-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.</td>\n",
       "      <td>Learning what to share between loosely related tasks</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1709.02349v2</td>\n",
       "      <td>9</td>\n",
       "      <td>We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.</td>\n",
       "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day            id  month  \\\n",
       "0    1  1802.00209v1      2   \n",
       "1   12  1603.03827v1      3   \n",
       "2    2  1606.00776v2      6   \n",
       "3   23  1705.08142v2      5   \n",
       "4    7  1709.02349v2      9   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           summary  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.   \n",
       "2  We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.   \n",
       "3                                                                                                                                                                                                                                                                                                       Multi-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.   \n",
       "\n",
       "                                                                                          title  \\\n",
       "0                                  Dual Recurrent Attention Units for Visual Question Answering   \n",
       "1      Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks   \n",
       "2  Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation   \n",
       "3                                          Learning what to share between loosely related tasks   \n",
       "4                                                         A Deep Reinforcement Learning Chatbot   \n",
       "\n",
       "   year  \n",
       "0  2018  \n",
       "1  2016  \n",
       "2  2016  \n",
       "3  2017  \n",
       "4  2017  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "data = pd.read_json('/usercode/arxivData.json')\n",
    "df = data.drop(columns=[\"author\", \"link\", 'tag'])\n",
    "print(\"Number of Machine Learning papers: \", df.id.unique().shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech Samek'}]</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.00209v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1802.00209v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1802.00209v1', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>2</td>\n",
       "      <td>We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.</td>\n",
       "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Dual Recurrent Attention Units for Visual Question Answering</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck Dernoncourt'}]</td>\n",
       "      <td>12</td>\n",
       "      <td>1603.03827v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1603.03827v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1603.03827v1', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim Klinger'}, {'name': 'Gerald Tesauro'}, {'name': 'Kartik Talamadupula'}, {'name': 'Bowen Zhou'}, {'name': 'Yoshua Bengio'}, {'name': 'Aaron Courville'}]</td>\n",
       "      <td>2</td>\n",
       "      <td>1606.00776v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1606.00776v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1606.00776v2', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>6</td>\n",
       "      <td>We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.5.1; I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachim Bingel'}, {'name': 'Isabelle Augenstein'}, {'name': 'Anders Søgaard'}]</td>\n",
       "      <td>23</td>\n",
       "      <td>1705.08142v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1705.08142v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1705.08142v2', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>5</td>\n",
       "      <td>Multi-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Learning what to share between loosely related tasks</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinnadhurai Sankar'}, {'name': 'Mathieu Germain'}, {'name': 'Saizheng Zhang'}, {'name': 'Zhouhan Lin'}, {'name': 'Sandeep Subramanian'}, {'name': 'Taesup Kim'}, {'name': 'Michael Pieper'}, {'name': 'Sarath Chandar'}, {'name': 'Nan Rosemary Ke'}, {'name': 'Sai Rajeshwar'}, {'name': 'Alexandre de Brebisson'}, {'name': 'Jose M. R. Sotelo'}, {'name': 'Dendi Suhubdy'}, {'name': 'Vincent Michalski'}, {'name': 'Alexandre Nguyen'}, {'name': 'Joelle Pineau'}, {'name': 'Yoshua Bengio'}]</td>\n",
       "      <td>7</td>\n",
       "      <td>1709.02349v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1709.02349v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1709.02349v2', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>9</td>\n",
       "      <td>We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.5.1; I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40995</th>\n",
       "      <td>[{'name': 'Vitaly Feldman'}, {'name': 'Pravesh Kothari'}, {'name': 'Jan Vondrák'}]</td>\n",
       "      <td>18</td>\n",
       "      <td>1404.4702v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.4702v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.4702v2', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>4</td>\n",
       "      <td>We study the complexity of learning and approximation of self-bounding\\nfunctions over the uniform distribution on the Boolean hypercube ${0,1}^n$.\\nInformally, a function $f:{0,1}^n \\rightarrow \\mathbb{R}$ is self-bounding if\\nfor every $x \\in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginal\\ndecreases in the value of the function at $x$. Self-bounding functions include\\nsuch well-known classes of functions as submodular and fractionally-subadditive\\n(XOS) functions. They were introduced by Boucheron et al. in the context of\\nconcentration of measure inequalities. Our main result is a nearly tight\\n$\\ell_1$-approximation of self-bounding functions by low-degree juntas.\\nSpecifically, all self-bounding functions can be $\\epsilon$-approximated in\\n$\\ell_1$ by a polynomial of degree $\\tilde{O}(1/\\epsilon)$ over\\n$2^{\\tilde{O}(1/\\epsilon)}$ variables. We show that both the degree and\\njunta-size are optimal up to logarithmic terms. Previous techniques considered\\nstronger $\\ell_2$ approximation and proved nearly tight bounds of\\n$\\Theta(1/\\epsilon^{2})$ on the degree and $2^{\\Theta(1/\\epsilon^2)}$ on the\\nnumber of variables. Our bounds rely on the analysis of noise stability of\\nself-bounding functions together with a stronger connection between noise\\nstability and $\\ell_1$ approximation by low-degree polynomials. This technique\\ncan also be used to get tighter bounds on $\\ell_1$ approximation by low-degree\\npolynomials and faster learning algorithm for halfspaces.\\n  These results lead to improved and in several cases almost tight bounds for\\nPAC and agnostic learning of self-bounding functions relative to the uniform\\ndistribution. In particular, assuming hardness of learning juntas, we show that\\nPAC and agnostic learning of self-bounding functions have complexity of\\n$n^{\\tilde{\\Theta}(1/\\epsilon)}$.</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Nearly Tight Bounds on $\\ell_1$ Approximation of Self-Bounding Functions</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40996</th>\n",
       "      <td>[{'name': 'Orly Avner'}, {'name': 'Shie Mannor'}]</td>\n",
       "      <td>22</td>\n",
       "      <td>1404.5421v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.5421v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.5421v1', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>4</td>\n",
       "      <td>We consider the problem of multiple users targeting the arms of a single\\nmulti-armed stochastic bandit. The motivation for this problem comes from\\ncognitive radio networks, where selfish users need to coexist without any side\\ncommunication between them, implicit cooperation or common control. Even the\\nnumber of users may be unknown and can vary as users join or leave the network.\\nWe propose an algorithm that combines an $\\epsilon$-greedy learning rule with a\\ncollision avoidance mechanism. We analyze its regret with respect to the\\nsystem-wide optimum and show that sub-linear regret can be obtained in this\\nsetting. Experiments show dramatic improvement compared to other algorithms for\\nthis setting.</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MA', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Concurrent bandits and cognitive radio networks</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40997</th>\n",
       "      <td>[{'name': 'Ran Zhao'}, {'name': 'Deanna Needell'}, {'name': 'Christopher Johansen'}, {'name': 'Jerry L. Grenard'}]</td>\n",
       "      <td>22</td>\n",
       "      <td>1404.5899v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.5899v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.5899v1', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>4</td>\n",
       "      <td>In this paper, we compare and analyze clustering methods with missing data in\\nhealth behavior research. In particular, we propose and analyze the use of\\ncompressive sensing's matrix completion along with spectral clustering to\\ncluster health related data. The empirical tests and real data results show\\nthat these methods can outperform standard methods like LPA and FIML, in terms\\nof lower misclassification rates in clustering and better matrix completion\\nperformance in missing data problems. According to our examination, a possible\\nexplanation of these improvements is that spectral clustering takes advantage\\nof high data dimension and compressive sensing methods utilize the\\nnear-to-low-rank property of health data.</td>\n",
       "      <td>[{'term': 'math.NA', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '62H30, 91C20, 94A08', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>A Comparison of Clustering and Missing Data Methods for Health Sciences</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40998</th>\n",
       "      <td>[{'name': 'Zongyan Huang'}, {'name': 'Matthew England'}, {'name': 'David Wilson'}, {'name': 'James H. Davenport'}, {'name': 'Lawrence C. Paulson'}, {'name': 'James Bridge'}]</td>\n",
       "      <td>25</td>\n",
       "      <td>1404.6369v1</td>\n",
       "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org/10.1007/978-3-319-08434-3_8', 'type': 'text/html', 'title': 'doi'}, {'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.6369v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.6369v1', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>4</td>\n",
       "      <td>Cylindrical algebraic decomposition(CAD) is a key tool in computational\\nalgebraic geometry, particularly for quantifier elimination over real-closed\\nfields. When using CAD, there is often a choice for the ordering placed on the\\nvariables. This can be important, with some problems infeasible with one\\nvariable ordering but easy with another. Machine learning is the process of\\nfitting a computer model to a complex function based on properties learned from\\nmeasured data. In this paper we use machine learning (specifically a support\\nvector machine) to select between heuristics for choosing a variable ordering,\\noutperforming each of the separate heuristics.</td>\n",
       "      <td>[{'term': 'cs.SC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '68W30, 68T05, O3C10', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.6', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>Applying machine learning to the problem of choosing a heuristic to\\n  select the variable ordering for cylindrical algebraic decomposition</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40999</th>\n",
       "      <td>[{'name': 'Imen Trabelsi'}, {'name': 'Dorra Ben Ayed'}]</td>\n",
       "      <td>27</td>\n",
       "      <td>1407.0380v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1407.0380v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1407.0380v1', 'type': 'application/pdf', 'title': 'pdf'}]</td>\n",
       "      <td>6</td>\n",
       "      <td>Several speaker identification systems are giving good performance with clean\\nspeech but are affected by the degradations introduced by noisy audio\\nconditions. To deal with this problem, we investigate the use of complementary\\ninformation at different levels for computing a combined match score for the\\nunknown speaker. In this work, we observe the effect of two supervised machine\\nlearning approaches including support vectors machines (SVM) and na\\\"ive bayes\\n(NB). We define two feature vector sets based on mel frequency cepstral\\ncoefficients (MFCC) and relative spectral perceptual linear predictive\\ncoefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture\\nModel (GMM). Several ways of combining these information sources give\\nsignificant improvements in a text-independent speaker identification task\\nusing a very large telephone degraded NTIMIT database.</td>\n",
       "      <td>[{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]</td>\n",
       "      <td>A Multi Level Data Fusion Approach for Speaker Identification on\\n  Telephone Speech</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            author  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [{'name': 'Ahmed Osman'}, {'name': 'Wojciech Samek'}]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [{'name': 'Ji Young Lee'}, {'name': 'Franck Dernoncourt'}]   \n",
       "2                                                                                                                                                                                                                                                                                                                                           [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim Klinger'}, {'name': 'Gerald Tesauro'}, {'name': 'Kartik Talamadupula'}, {'name': 'Bowen Zhou'}, {'name': 'Yoshua Bengio'}, {'name': 'Aaron Courville'}]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                           [{'name': 'Sebastian Ruder'}, {'name': 'Joachim Bingel'}, {'name': 'Isabelle Augenstein'}, {'name': 'Anders Søgaard'}]   \n",
       "4      [{'name': 'Iulian V. Serban'}, {'name': 'Chinnadhurai Sankar'}, {'name': 'Mathieu Germain'}, {'name': 'Saizheng Zhang'}, {'name': 'Zhouhan Lin'}, {'name': 'Sandeep Subramanian'}, {'name': 'Taesup Kim'}, {'name': 'Michael Pieper'}, {'name': 'Sarath Chandar'}, {'name': 'Nan Rosemary Ke'}, {'name': 'Sai Rajeshwar'}, {'name': 'Alexandre de Brebisson'}, {'name': 'Jose M. R. Sotelo'}, {'name': 'Dendi Suhubdy'}, {'name': 'Vincent Michalski'}, {'name': 'Alexandre Nguyen'}, {'name': 'Joelle Pineau'}, {'name': 'Yoshua Bengio'}]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...   \n",
       "40995                                                                                                                                                                                                                                                                                                                                                                                                                                                           [{'name': 'Vitaly Feldman'}, {'name': 'Pravesh Kothari'}, {'name': 'Jan Vondrák'}]   \n",
       "40996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [{'name': 'Orly Avner'}, {'name': 'Shie Mannor'}]   \n",
       "40997                                                                                                                                                                                                                                                                                                                                                                                                                           [{'name': 'Ran Zhao'}, {'name': 'Deanna Needell'}, {'name': 'Christopher Johansen'}, {'name': 'Jerry L. Grenard'}]   \n",
       "40998                                                                                                                                                                                                                                                                                                                                                                [{'name': 'Zongyan Huang'}, {'name': 'Matthew England'}, {'name': 'David Wilson'}, {'name': 'James H. Davenport'}, {'name': 'Lawrence C. Paulson'}, {'name': 'James Bridge'}]   \n",
       "40999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [{'name': 'Imen Trabelsi'}, {'name': 'Dorra Ben Ayed'}]   \n",
       "\n",
       "       day            id  \\\n",
       "0        1  1802.00209v1   \n",
       "1       12  1603.03827v1   \n",
       "2        2  1606.00776v2   \n",
       "3       23  1705.08142v2   \n",
       "4        7  1709.02349v2   \n",
       "...    ...           ...   \n",
       "40995   18   1404.4702v2   \n",
       "40996   22   1404.5421v1   \n",
       "40997   22   1404.5899v1   \n",
       "40998   25   1404.6369v1   \n",
       "40999   27   1407.0380v1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                       link  \\\n",
       "0                                                                                                                      [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1802.00209v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1802.00209v1', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "1                                                                                                                      [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1603.03827v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1603.03827v1', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "2                                                                                                                      [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1606.00776v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1606.00776v2', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "3                                                                                                                      [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1705.08142v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1705.08142v2', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "4                                                                                                                      [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1709.02349v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1709.02349v2', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "...                                                                                                                                                                                                                                                                                                                     ...   \n",
       "40995                                                                                                                    [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.4702v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.4702v2', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "40996                                                                                                                    [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.5421v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.5421v1', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "40997                                                                                                                    [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.5899v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.5899v1', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "40998  [{'rel': 'related', 'href': 'http://dx.doi.org/10.1007/978-3-319-08434-3_8', 'type': 'text/html', 'title': 'doi'}, {'rel': 'alternate', 'href': 'http://arxiv.org/abs/1404.6369v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1404.6369v1', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "40999                                                                                                                    [{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1407.0380v1', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1407.0380v1', 'type': 'application/pdf', 'title': 'pdf'}]   \n",
       "\n",
       "       month  \\\n",
       "0          2   \n",
       "1          3   \n",
       "2          6   \n",
       "3          5   \n",
       "4          9   \n",
       "...      ...   \n",
       "40995      4   \n",
       "40996      4   \n",
       "40997      4   \n",
       "40998      4   \n",
       "40999      6   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       summary  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                              We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Multi-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "40995  We study the complexity of learning and approximation of self-bounding\\nfunctions over the uniform distribution on the Boolean hypercube ${0,1}^n$.\\nInformally, a function $f:{0,1}^n \\rightarrow \\mathbb{R}$ is self-bounding if\\nfor every $x \\in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginal\\ndecreases in the value of the function at $x$. Self-bounding functions include\\nsuch well-known classes of functions as submodular and fractionally-subadditive\\n(XOS) functions. They were introduced by Boucheron et al. in the context of\\nconcentration of measure inequalities. Our main result is a nearly tight\\n$\\ell_1$-approximation of self-bounding functions by low-degree juntas.\\nSpecifically, all self-bounding functions can be $\\epsilon$-approximated in\\n$\\ell_1$ by a polynomial of degree $\\tilde{O}(1/\\epsilon)$ over\\n$2^{\\tilde{O}(1/\\epsilon)}$ variables. We show that both the degree and\\njunta-size are optimal up to logarithmic terms. Previous techniques considered\\nstronger $\\ell_2$ approximation and proved nearly tight bounds of\\n$\\Theta(1/\\epsilon^{2})$ on the degree and $2^{\\Theta(1/\\epsilon^2)}$ on the\\nnumber of variables. Our bounds rely on the analysis of noise stability of\\nself-bounding functions together with a stronger connection between noise\\nstability and $\\ell_1$ approximation by low-degree polynomials. This technique\\ncan also be used to get tighter bounds on $\\ell_1$ approximation by low-degree\\npolynomials and faster learning algorithm for halfspaces.\\n  These results lead to improved and in several cases almost tight bounds for\\nPAC and agnostic learning of self-bounding functions relative to the uniform\\ndistribution. In particular, assuming hardness of learning juntas, we show that\\nPAC and agnostic learning of self-bounding functions have complexity of\\n$n^{\\tilde{\\Theta}(1/\\epsilon)}$.   \n",
       "40996                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We consider the problem of multiple users targeting the arms of a single\\nmulti-armed stochastic bandit. The motivation for this problem comes from\\ncognitive radio networks, where selfish users need to coexist without any side\\ncommunication between them, implicit cooperation or common control. Even the\\nnumber of users may be unknown and can vary as users join or leave the network.\\nWe propose an algorithm that combines an $\\epsilon$-greedy learning rule with a\\ncollision avoidance mechanism. We analyze its regret with respect to the\\nsystem-wide optimum and show that sub-linear regret can be obtained in this\\nsetting. Experiments show dramatic improvement compared to other algorithms for\\nthis setting.   \n",
       "40997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In this paper, we compare and analyze clustering methods with missing data in\\nhealth behavior research. In particular, we propose and analyze the use of\\ncompressive sensing's matrix completion along with spectral clustering to\\ncluster health related data. The empirical tests and real data results show\\nthat these methods can outperform standard methods like LPA and FIML, in terms\\nof lower misclassification rates in clustering and better matrix completion\\nperformance in missing data problems. According to our examination, a possible\\nexplanation of these improvements is that spectral clustering takes advantage\\nof high data dimension and compressive sensing methods utilize the\\nnear-to-low-rank property of health data.   \n",
       "40998                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Cylindrical algebraic decomposition(CAD) is a key tool in computational\\nalgebraic geometry, particularly for quantifier elimination over real-closed\\nfields. When using CAD, there is often a choice for the ordering placed on the\\nvariables. This can be important, with some problems infeasible with one\\nvariable ordering but easy with another. Machine learning is the process of\\nfitting a computer model to a complex function based on properties learned from\\nmeasured data. In this paper we use machine learning (specifically a support\\nvector machine) to select between heuristics for choosing a variable ordering,\\noutperforming each of the separate heuristics.   \n",
       "40999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Several speaker identification systems are giving good performance with clean\\nspeech but are affected by the degradations introduced by noisy audio\\nconditions. To deal with this problem, we investigate the use of complementary\\ninformation at different levels for computing a combined match score for the\\nunknown speaker. In this work, we observe the effect of two supervised machine\\nlearning approaches including support vectors machines (SVM) and na\\\"ive bayes\\n(NB). We define two feature vector sets based on mel frequency cepstral\\ncoefficients (MFCC) and relative spectral perceptual linear predictive\\ncoefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture\\nModel (GMM). Several ways of combining these information sources give\\nsignificant improvements in a text-independent speaker identification task\\nusing a very large telephone degraded NTIMIT database.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           tag  \\\n",
       "0                                                                                          [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "1                                                                                          [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "2      [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.5.1; I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "3                                                                                          [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "4      [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.5.1; I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "40995                                                                                                                                                                                                                                                                                                                               [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "40996                                                                                                                                                                                                                                                                                                                               [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MA', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "40997                                                                                                                                                                                                                                  [{'term': 'math.NA', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '62H30, 91C20, 94A08', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "40998                                                                                                                                                       [{'term': 'cs.SC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '68W30, 68T05, O3C10', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.6', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "40999                                                                                                                                                                                                                                                                                                                               [{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]   \n",
       "\n",
       "                                                                                                                                             title  \\\n",
       "0                                                                                     Dual Recurrent Attention Units for Visual Question Answering   \n",
       "1                                                         Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks   \n",
       "2                                                     Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation   \n",
       "3                                                                                             Learning what to share between loosely related tasks   \n",
       "4                                                                                                            A Deep Reinforcement Learning Chatbot   \n",
       "...                                                                                                                                            ...   \n",
       "40995                                                                     Nearly Tight Bounds on $\\ell_1$ Approximation of Self-Bounding Functions   \n",
       "40996                                                                                              Concurrent bandits and cognitive radio networks   \n",
       "40997                                                                      A Comparison of Clustering and Missing Data Methods for Health Sciences   \n",
       "40998  Applying machine learning to the problem of choosing a heuristic to\\n  select the variable ordering for cylindrical algebraic decomposition   \n",
       "40999                                                         A Multi Level Data Fusion Approach for Speaker Identification on\\n  Telephone Speech   \n",
       "\n",
       "       year  \n",
       "0      2018  \n",
       "1      2016  \n",
       "2      2016  \n",
       "3      2017  \n",
       "4      2017  \n",
       "...     ...  \n",
       "40995  2014  \n",
       "40996  2014  \n",
       "40997  2014  \n",
       "40998  2014  \n",
       "40999  2014  \n",
       "\n",
       "[41000 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s3bRy6lMGCE"
   },
   "source": [
    "## Task 3: Retrieve the Model\n",
    "\n",
    "In this task, you’ll load the DistilBERT model using the sentence_transformers library. The sentence_transformers library allows us to use \n",
    "Transformer-based models from Huggingface, that are fine-tuned to generate semantically meaningful embedding matrices given natural language. \n",
    "The DistilBERT model is much smaller than the BERT model while having comparable performance and therefore are more suitable for our use case.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Use the SentenceTransformer() method from the sentence_transformers library to load a DistilBERT model. This method takes the name of the\n",
    "Transformer-based model to load as a parameter.\n",
    "                                                                                                                            \n",
    "2. Move the model to the GPU if it is available. Print the device on which the model is located.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754,
     "referenced_widgets": [
      "53bd1728e57a400aac0c684bba95ccb1",
      "220b3167108c45aba1b72916293bb3a3",
      "4b92df9fb7294c2da58fe1d2ddc610f4",
      "0097d999ff37437eaefc09d25e51ef1c",
      "bfe7598cb6584390981f157f4d7db1c3",
      "b9700ea0ef3e4461a3c128d392838986",
      "6af3912429794e65ab27e848d70d48c5",
      "12f2b0efa8224cde834bd57882aea762",
      "100afbd6a4e841149873701151872b03",
      "1ed8f2bd24de42088f96181cd5253f72",
      "fb366cdce58c478f8a7e69a336d7d70e",
      "007d212316024b57adcac7871e445d10",
      "b5df03efae4f45dbb912cb8529154304",
      "d798a2cb981e4bea896447a54eec09cc",
      "0198a121ecfc40a7b6e9e6425186e4cd",
      "59b33c71bdbe41e893c88c86b2dc704b",
      "3709d5063ff242fda5150b1c8c6c4664",
      "d20db335687b4289a9e0196710fa9f8e",
      "da94a6e39c0f42c8809c4533d87dbbba",
      "1925808c688544bf9d071f588633b250",
      "7bb104f572e445ffb01a7773fbd305e4",
      "83741ea2737640078f21089c3dda2166",
      "315f2c119f3d4eb792f4ce606290a53c",
      "429ccab3671546d3a62574332fa644c6",
      "8cd3e20119fe4fa2bde2606e310b507c",
      "5186fe054bd54b8f8badddb6caa9bd03",
      "4744d955f20f4887aff2345b15347ae5",
      "c010bca4b46c426cb4d75734ee4d213e",
      "1cc90111ab874b5cb4cbaee57f90129b",
      "896f716571074df9a0b4505da5a655ca",
      "a93544672d484e55a177c8ddb4723c19",
      "edb01c355cbc4e1eac31593ac2c8a555",
      "d320f358e78c4d0a9bd0b5dc082a40b4",
      "8cd9b53d885246c7aa82a2ab03f6357b",
      "8edc778a5055444ea63f48545e8089aa",
      "abb09c1f043646b98a862ab673edb310",
      "d98d3d5e8e4f4a96a99fba4abe773104",
      "8658a29975974246af3df68e5e52198d",
      "1f598a0edd544bd4b39faac1de090b9d",
      "7d5456b0ea6d4cbf95cd93e4db33485a",
      "41869a9881ae42e7af2b7cef43a56f66",
      "196c48db830f4a3bb2fcac35f673938d",
      "734fc4060cc54b3eb64bfa32bd0975ff",
      "6a2abb6db22348bca9b9da53b0320e0d",
      "1f18b26b1fc84db0829bc2cbbc63ff10",
      "9e3677374c19471a84bb78c3dcddfca1",
      "bc31a80accf844c3a6a2c89b673cef9b",
      "2bab636ec40f44a8b7b8c7d4b1ae5a1f",
      "672f6e70eef04b7dbf4ae5b78f132845",
      "fbc3e919348e4292af76e770fdd6f06f",
      "3477b89bd24d4f6f94a57741ce0f744b",
      "4ea10297a23249b7b1f4843e521e27de",
      "deff838ffb814fbf87e9f202a8d94e00",
      "20072133987b4bcc88de1404d169bf9d",
      "1076f43cda864471b019d9791950b011",
      "5b86e51a9ddd43478cb4ff0ca45d8724",
      "b716708dc7474328992c5b26697aa4e9",
      "df7d9b5f58d6435187360a31e79b176a",
      "44e9016964fe4c49ad77cd92a24c137b",
      "2ae618c6e8084fde857a2d4b00fa5a86",
      "7a745e2bcba94554880c67aaa83c158b",
      "54a15fdc18fb40029ebf9839225c40fc",
      "f4315e4e636c40f1a9205524642c0d7b",
      "d32bfdf2d9b2486089c2f6f81535d49e",
      "bdb19005a4e94f8a90c29260964304cc",
      "33c3c1ebaa6f4cd6a352c9787f8fffe5",
      "1288bd5eda084e5c949937814888caca",
      "6a7ab4e3aa374789b9a30a973951733f",
      "d38bba863f9e404a9b865c0a2c03311c",
      "0dee9424ad704d17810409b0f5582fb4",
      "6abeec7be8164db79447625ad5969002",
      "1249d6a245704760876d86b16c55aeb2",
      "a4d6f98130a142588ce66414279823ac",
      "37d30d9227094847b02c3f205904f1a8",
      "3e1885f62864416aa8ac36c8fca91277",
      "cab5e02948ca4895a51b911f6709b3e3",
      "b29f27e7283d468b9ae6f38dd29b0c6d",
      "cad3b086340944a59375ece310d3a705",
      "d028083f47834b3491a025e1361757da",
      "be20d532415c4f82b827ea0ea1cb59b8",
      "780e3bdd45a5495592cc5fb44780f88c",
      "4b15909c8b004279a389902115f89421",
      "e797accb6ab54ee7b51bfdb525c4cccb",
      "696574fd545c4b55be32ea07e094b7d6",
      "770a8586da4844dc84366444a8f65637",
      "14257bc929f7424092463b34d0fc4a61",
      "aa5ecefecef34cdba58ff4d4092ced56",
      "732122923ebd4a1e9a59114a75212733",
      "57fbf00307494309900693460b93ee45",
      "d0716fb9e26f4673b23c912b0720e2ce",
      "3db9e74d662c491da6e0314db32c754d",
      "753b311297504d2e98c09c426710f3f7",
      "ad6354fed78e4209a0eb7e5b0ef81fcc",
      "3b5d0c5b41114272b23c5aaf6e2d5cf9",
      "7ede626c3dde463b822a84e75a23face",
      "3c995640f16e4ff4aeede34f341728ee",
      "5d8b13fbee904d49bc50778fee62afe7",
      "08725b73d1064201b2a5ae59f71229f1",
      "fcdefdf5882c44b995cadce70b4d1211",
      "89e895bdbb0d489592bbc23a19da5e09",
      "3fe0745adab24c6a82877ca471e5c32f",
      "8795346c4376409d9b5126236b8ecee6",
      "15cd689a923f423087fb6945f7ba22d2",
      "fa8540d5c08145e9b51cadbdc063b715",
      "09227dfc526c47929a938864fe2f7ee7",
      "1673f62c33924d6dbe159b665e092044",
      "f56e474aa3734728b699c6c2ed6c709e",
      "ef36ada088a24063b6ed058c2faf133a",
      "03c465d2324245aea22fafe3753999c6",
      "ffcf6844c579427da16fae54fddf20dc",
      "75ce838fcfc745dca5fd9da268478d01",
      "7d66b7cdf9624c56ae3b628399dc7c72",
      "e044383b011f4863a05fb95770b33781",
      "3a377053c7da4c538145213bd5bf5652",
      "4e212c53e4464720bbb8f2a5d22f7bf8",
      "4a7b562563a849ed9ea2c7c159c305b1",
      "97644a9c4fe148f0a1c62532d4a6fe5f",
      "0be8b9bc5d4c417ab28216732e69bd57",
      "f28ac8e1d9d648b2b285be59f4a353c1",
      "93fbf4a4c01847109b23b94c1561907e",
      "bb3f9420a43c490985220119e79e0dbd",
      "93cc574a16bb4ee3aa37d7bb00807b95",
      "b0131633e5184851bf231fef45d8d942",
      "2e7ff5c089c840c298df34c9085e7288",
      "24f46851a05d4f0ba7cedf9fe6b854c1",
      "10feddb6ec244d259c55144943e51987",
      "ddd1482ada5843bcbee039f46430e3dc",
      "28ee7786ee284adfb4b6cba3440a7a99",
      "a728a2ef35d447f890603d9c9f79f15d",
      "77368f4321704269ad4dc3c820a1a8f4",
      "85a1bbf88ecd481bb0f6b8ee019dd495",
      "7cf2b46a8de34dc8b6aead85500ab6dd"
     ]
    },
    "id": "PjF6CrwUVyRx",
    "outputId": "5808ff5c-7a37-49df-9c7b-417798833e6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes:   0%|          | 0.00/399 [00:00<?, ?B/s]\r",
      "Downloading .gitattributes: 100%|██████████| 399/399 [00:00<00:00, 34.9kB/s]\n",
      "\r",
      "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]\r",
      "Downloading 1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 18.4kB/s]\n",
      "\r",
      "Downloading README.md:   0%|          | 0.00/4.05k [00:00<?, ?B/s]\r",
      "Downloading README.md: 100%|██████████| 4.05k/4.05k [00:00<00:00, 807kB/s]\n",
      "\r",
      "Downloading config.json:   0%|          | 0.00/555 [00:00<?, ?B/s]\r",
      "Downloading config.json: 100%|██████████| 555/555 [00:00<00:00, 121kB/s]\n",
      "\r",
      "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]\r",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 20.8kB/s]\n",
      "\r",
      "Downloading model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]\r",
      "Downloading model.safetensors:   4%|▍         | 10.5M/265M [00:00<00:03, 71.6MB/s]\r",
      "Downloading model.safetensors:  12%|█▏        | 31.5M/265M [00:00<00:01, 123MB/s] \r",
      "Downloading model.safetensors:  20%|█▉        | 52.4M/265M [00:00<00:01, 152MB/s]\r",
      "Downloading model.safetensors:  28%|██▊       | 73.4M/265M [00:00<00:01, 167MB/s]\r",
      "Downloading model.safetensors:  36%|███▌      | 94.4M/265M [00:00<00:00, 177MB/s]\r",
      "Downloading model.safetensors:  43%|████▎     | 115M/265M [00:00<00:00, 183MB/s] \r",
      "Downloading model.safetensors:  51%|█████▏    | 136M/265M [00:00<00:00, 187MB/s]\r",
      "Downloading model.safetensors:  59%|█████▉    | 157M/265M [00:00<00:00, 190MB/s]\r",
      "Downloading model.safetensors:  67%|██████▋   | 178M/265M [00:01<00:00, 187MB/s]\r",
      "Downloading model.safetensors:  75%|███████▌  | 199M/265M [00:01<00:00, 191MB/s]\r",
      "Downloading model.safetensors:  83%|████████▎ | 220M/265M [00:01<00:00, 194MB/s]\r",
      "Downloading model.safetensors:  91%|█████████ | 241M/265M [00:01<00:00, 195MB/s]\r",
      "Downloading model.safetensors:  99%|█████████▊| 262M/265M [00:01<00:00, 195MB/s]\r",
      "Downloading model.safetensors: 100%|██████████| 265M/265M [00:01<00:00, 178MB/s]\n",
      "\r",
      "Downloading model.onnx:   0%|          | 0.00/266M [00:00<?, ?B/s]\r",
      "Downloading model.onnx:   4%|▍         | 10.5M/266M [00:00<00:03, 74.0MB/s]\r",
      "Downloading model.onnx:  12%|█▏        | 31.5M/266M [00:00<00:01, 120MB/s] \r",
      "Downloading model.onnx:  20%|█▉        | 52.4M/266M [00:00<00:01, 141MB/s]\r",
      "Downloading model.onnx:  28%|██▊       | 73.4M/266M [00:00<00:01, 158MB/s]\r",
      "Downloading model.onnx:  36%|███▌      | 94.4M/266M [00:00<00:00, 172MB/s]\r",
      "Downloading model.onnx:  43%|████▎     | 115M/266M [00:00<00:00, 178MB/s] \r",
      "Downloading model.onnx:  51%|█████▏    | 136M/266M [00:00<00:00, 174MB/s]\r",
      "Downloading model.onnx:  59%|█████▉    | 157M/266M [00:00<00:00, 178MB/s]\r",
      "Downloading model.onnx:  67%|██████▋   | 178M/266M [00:01<00:00, 180MB/s]\r",
      "Downloading model.onnx:  75%|███████▌  | 199M/266M [00:01<00:00, 183MB/s]\r",
      "Downloading model.onnx:  83%|████████▎ | 220M/266M [00:01<00:00, 184MB/s]\r",
      "Downloading model.onnx:  91%|█████████ | 241M/266M [00:01<00:00, 182MB/s]\r",
      "Downloading model.onnx:  99%|█████████▊| 262M/266M [00:01<00:00, 188MB/s]\r",
      "Downloading model.onnx: 100%|██████████| 266M/266M [00:01<00:00, 170MB/s]\n",
      "\r",
      "Downloading model_O1.onnx:   0%|          | 0.00/266M [00:00<?, ?B/s]\r",
      "Downloading model_O1.onnx:   4%|▍         | 10.5M/266M [00:00<00:03, 73.0MB/s]\r",
      "Downloading model_O1.onnx:  12%|█▏        | 31.5M/266M [00:00<00:01, 131MB/s] \r",
      "Downloading model_O1.onnx:  20%|█▉        | 52.4M/266M [00:00<00:01, 158MB/s]\r",
      "Downloading model_O1.onnx:  28%|██▊       | 73.4M/266M [00:00<00:01, 163MB/s]\r",
      "Downloading model_O1.onnx:  36%|███▌      | 94.4M/266M [00:00<00:00, 172MB/s]\r",
      "Downloading model_O1.onnx:  43%|████▎     | 115M/266M [00:00<00:00, 179MB/s] \r",
      "Downloading model_O1.onnx:  51%|█████▏    | 136M/266M [00:00<00:00, 183MB/s]\r",
      "Downloading model_O1.onnx:  59%|█████▉    | 157M/266M [00:00<00:00, 185MB/s]\r",
      "Downloading model_O1.onnx:  67%|██████▋   | 178M/266M [00:01<00:00, 190MB/s]\r",
      "Downloading model_O1.onnx:  75%|███████▌  | 199M/266M [00:01<00:00, 186MB/s]\r",
      "Downloading model_O1.onnx:  83%|████████▎ | 220M/266M [00:01<00:00, 189MB/s]\r",
      "Downloading model_O1.onnx:  91%|█████████ | 241M/266M [00:01<00:00, 191MB/s]\r",
      "Downloading model_O1.onnx:  99%|█████████▊| 262M/266M [00:01<00:00, 191MB/s]\r",
      "Downloading model_O1.onnx: 100%|██████████| 266M/266M [00:01<00:00, 176MB/s]\n",
      "\r",
      "Downloading model_O2.onnx:   0%|          | 0.00/265M [00:00<?, ?B/s]\r",
      "Downloading model_O2.onnx:   4%|▍         | 10.5M/265M [00:00<00:04, 55.2MB/s]\r",
      "Downloading model_O2.onnx:  12%|█▏        | 31.5M/265M [00:00<00:03, 73.3MB/s]\r",
      "Downloading model_O2.onnx:  20%|█▉        | 52.4M/265M [00:00<00:01, 108MB/s] \r",
      "Downloading model_O2.onnx:  28%|██▊       | 73.4M/265M [00:00<00:01, 134MB/s]\r",
      "Downloading model_O2.onnx:  36%|███▌      | 94.4M/265M [00:00<00:01, 150MB/s]\r",
      "Downloading model_O2.onnx:  43%|████▎     | 115M/265M [00:00<00:00, 163MB/s] \r",
      "Downloading model_O2.onnx:  51%|█████▏    | 136M/265M [00:01<00:00, 158MB/s]\r",
      "Downloading model_O2.onnx:  59%|█████▉    | 157M/265M [00:01<00:00, 167MB/s]\r",
      "Downloading model_O2.onnx:  67%|██████▋   | 178M/265M [00:01<00:00, 175MB/s]\r",
      "Downloading model_O2.onnx:  75%|███████▌  | 199M/265M [00:01<00:00, 179MB/s]\r",
      "Downloading model_O2.onnx:  83%|████████▎ | 220M/265M [00:01<00:00, 173MB/s]\r",
      "Downloading model_O2.onnx:  91%|█████████ | 241M/265M [00:01<00:00, 165MB/s]\r",
      "Downloading model_O2.onnx:  99%|█████████▊| 262M/265M [00:01<00:00, 172MB/s]\r",
      "Downloading model_O2.onnx: 100%|██████████| 265M/265M [00:01<00:00, 150MB/s]\n",
      "\r",
      "Downloading model_O3.onnx:   0%|          | 0.00/265M [00:00<?, ?B/s]\r",
      "Downloading model_O3.onnx:   4%|▍         | 10.5M/265M [00:00<00:03, 75.7MB/s]\r",
      "Downloading model_O3.onnx:  12%|█▏        | 31.5M/265M [00:00<00:01, 126MB/s] \r",
      "Downloading model_O3.onnx:  20%|█▉        | 52.4M/265M [00:00<00:01, 152MB/s]\r",
      "Downloading model_O3.onnx:  28%|██▊       | 73.4M/265M [00:00<00:01, 163MB/s]\r",
      "Downloading model_O3.onnx:  36%|███▌      | 94.4M/265M [00:00<00:00, 174MB/s]\r",
      "Downloading model_O3.onnx:  43%|████▎     | 115M/265M [00:00<00:00, 180MB/s] \r",
      "Downloading model_O3.onnx:  51%|█████▏    | 136M/265M [00:00<00:00, 181MB/s]\r",
      "Downloading model_O3.onnx:  59%|█████▉    | 157M/265M [00:00<00:00, 186MB/s]\r",
      "Downloading model_O3.onnx:  67%|██████▋   | 178M/265M [00:01<00:00, 177MB/s]\r",
      "Downloading model_O3.onnx:  75%|███████▌  | 199M/265M [00:01<00:00, 182MB/s]\r",
      "Downloading model_O3.onnx:  83%|████████▎ | 220M/265M [00:01<00:00, 184MB/s]\r",
      "Downloading model_O3.onnx:  91%|█████████ | 241M/265M [00:01<00:00, 187MB/s]\r",
      "Downloading model_O3.onnx:  99%|█████████▊| 262M/265M [00:01<00:00, 170MB/s]\r",
      "Downloading model_O3.onnx: 100%|██████████| 265M/265M [00:01<00:00, 167MB/s]\n",
      "\r",
      "Downloading model_O4.onnx:   0%|          | 0.00/133M [00:00<?, ?B/s]\r",
      "Downloading model_O4.onnx:   8%|▊         | 10.5M/133M [00:00<00:01, 70.5MB/s]\r",
      "Downloading model_O4.onnx:  24%|██▎       | 31.5M/133M [00:00<00:00, 121MB/s] \r",
      "Downloading model_O4.onnx:  39%|███▉      | 52.4M/133M [00:00<00:00, 152MB/s]\r",
      "Downloading model_O4.onnx:  55%|█████▌    | 73.4M/133M [00:00<00:00, 167MB/s]\r",
      "Downloading model_O4.onnx:  71%|███████   | 94.4M/133M [00:00<00:00, 168MB/s]\r",
      "Downloading model_O4.onnx:  87%|████████▋ | 115M/133M [00:00<00:00, 170MB/s] \r",
      "Downloading model_O4.onnx: 100%|██████████| 133M/133M [00:00<00:00, 158MB/s]\n",
      "\r",
      "Downloading model_qint8_arm64.onnx:   0%|          | 0.00/67.0M [00:00<?, ?B/s]\r",
      "Downloading model_qint8_arm64.onnx:  16%|█▌        | 10.5M/67.0M [00:00<00:00, 71.5MB/s]\r",
      "Downloading model_qint8_arm64.onnx:  47%|████▋     | 31.5M/67.0M [00:00<00:00, 121MB/s] \r",
      "Downloading model_qint8_arm64.onnx:  78%|███████▊  | 52.4M/67.0M [00:00<00:00, 139MB/s]\r",
      "Downloading model_qint8_arm64.onnx: 100%|██████████| 67.0M/67.0M [00:00<00:00, 130MB/s]\n",
      "\r",
      "Downloading (…)el_qint8_avx512.onnx:   0%|          | 0.00/67.0M [00:00<?, ?B/s]\r",
      "Downloading (…)el_qint8_avx512.onnx:  16%|█▌        | 10.5M/67.0M [00:00<00:01, 54.6MB/s]\r",
      "Downloading (…)el_qint8_avx512.onnx:  31%|███▏      | 21.0M/67.0M [00:00<00:00, 64.0MB/s]\r",
      "Downloading (…)el_qint8_avx512.onnx:  63%|██████▎   | 41.9M/67.0M [00:00<00:00, 95.1MB/s]\r",
      "Downloading (…)el_qint8_avx512.onnx:  78%|███████▊  | 52.4M/67.0M [00:00<00:00, 84.2MB/s]\r",
      "Downloading (…)el_qint8_avx512.onnx:  94%|█████████▍| 62.9M/67.0M [00:00<00:00, 81.7MB/s]\r",
      "Downloading (…)el_qint8_avx512.onnx: 100%|██████████| 67.0M/67.0M [00:00<00:00, 68.2MB/s]\n",
      "\r",
      "Downloading (…)nt8_avx512_vnni.onnx:   0%|          | 0.00/67.0M [00:00<?, ?B/s]\r",
      "Downloading (…)nt8_avx512_vnni.onnx:  16%|█▌        | 10.5M/67.0M [00:00<00:01, 55.2MB/s]\r",
      "Downloading (…)nt8_avx512_vnni.onnx:  47%|████▋     | 31.5M/67.0M [00:00<00:00, 98.2MB/s]\r",
      "Downloading (…)nt8_avx512_vnni.onnx:  78%|███████▊  | 52.4M/67.0M [00:00<00:00, 122MB/s] \r",
      "Downloading (…)nt8_avx512_vnni.onnx: 100%|██████████| 67.0M/67.0M [00:00<00:00, 124MB/s]\r",
      "Downloading (…)nt8_avx512_vnni.onnx: 100%|██████████| 67.0M/67.0M [00:00<00:00, 111MB/s]\n",
      "\r",
      "Downloading model_quint8_avx2.onnx:   0%|          | 0.00/67.0M [00:00<?, ?B/s]\r",
      "Downloading model_quint8_avx2.onnx:  16%|█▌        | 10.5M/67.0M [00:00<00:00, 69.8MB/s]\r",
      "Downloading model_quint8_avx2.onnx:  47%|████▋     | 31.5M/67.0M [00:00<00:00, 99.4MB/s]\r",
      "Downloading model_quint8_avx2.onnx:  78%|███████▊  | 52.4M/67.0M [00:00<00:00, 70.3MB/s]\r",
      "Downloading model_quint8_avx2.onnx:  94%|█████████▍| 62.9M/67.0M [00:00<00:00, 60.7MB/s]\r",
      "Downloading model_quint8_avx2.onnx: 100%|██████████| 67.0M/67.0M [00:01<00:00, 65.7MB/s]\n",
      "\r",
      "Downloading openvino_model.bin:   0%|          | 0.00/265M [00:00<?, ?B/s]\r",
      "Downloading openvino_model.bin:   4%|▍         | 10.5M/265M [00:00<00:03, 71.8MB/s]\r",
      "Downloading openvino_model.bin:  12%|█▏        | 31.5M/265M [00:00<00:01, 117MB/s] \r",
      "Downloading openvino_model.bin:  20%|█▉        | 52.4M/265M [00:00<00:01, 129MB/s]\r",
      "Downloading openvino_model.bin:  28%|██▊       | 73.4M/265M [00:00<00:01, 139MB/s]\r",
      "Downloading openvino_model.bin:  36%|███▌      | 94.4M/265M [00:00<00:01, 139MB/s]\r",
      "Downloading openvino_model.bin:  43%|████▎     | 115M/265M [00:00<00:01, 147MB/s] \r",
      "Downloading openvino_model.bin:  51%|█████▏    | 136M/265M [00:00<00:00, 161MB/s]\r",
      "Downloading openvino_model.bin:  59%|█████▉    | 157M/265M [00:01<00:00, 158MB/s]\r",
      "Downloading openvino_model.bin:  67%|██████▋   | 178M/265M [00:01<00:00, 170MB/s]\r",
      "Downloading openvino_model.bin:  75%|███████▌  | 199M/265M [00:01<00:00, 178MB/s]\r",
      "Downloading openvino_model.bin:  83%|████████▎ | 220M/265M [00:01<00:00, 143MB/s]\r",
      "Downloading openvino_model.bin:  91%|█████████ | 241M/265M [00:01<00:00, 156MB/s]\r",
      "Downloading openvino_model.bin:  99%|█████████▉| 262M/265M [00:01<00:00, 166MB/s]\r",
      "Downloading openvino_model.bin: 100%|██████████| 265M/265M [00:01<00:00, 151MB/s]\n",
      "\r",
      "Downloading (…)o/openvino_model.xml:   0%|          | 0.00/217k [00:00<?, ?B/s]\r",
      "Downloading (…)o/openvino_model.xml: 100%|██████████| 217k/217k [00:00<00:00, 5.97MB/s]\n",
      "\r",
      "Downloading pytorch_model.bin:   0%|          | 0.00/265M [00:00<?, ?B/s]\r",
      "Downloading pytorch_model.bin:   4%|▍         | 10.5M/265M [00:00<00:02, 91.8MB/s]\r",
      "Downloading pytorch_model.bin:  12%|█▏        | 31.5M/265M [00:00<00:01, 150MB/s] \r",
      "Downloading pytorch_model.bin:  20%|█▉        | 52.4M/265M [00:00<00:01, 172MB/s]\r",
      "Downloading pytorch_model.bin:  28%|██▊       | 73.4M/265M [00:00<00:01, 183MB/s]\r",
      "Downloading pytorch_model.bin:  36%|███▌      | 94.4M/265M [00:00<00:00, 185MB/s]\r",
      "Downloading pytorch_model.bin:  43%|████▎     | 115M/265M [00:00<00:00, 190MB/s] \r",
      "Downloading pytorch_model.bin:  51%|█████▏    | 136M/265M [00:00<00:00, 192MB/s]\r",
      "Downloading pytorch_model.bin:  59%|█████▉    | 157M/265M [00:00<00:00, 191MB/s]\r",
      "Downloading pytorch_model.bin:  67%|██████▋   | 178M/265M [00:01<00:00, 178MB/s]\r",
      "Downloading pytorch_model.bin:  75%|███████▌  | 199M/265M [00:01<00:00, 181MB/s]\r",
      "Downloading pytorch_model.bin:  83%|████████▎ | 220M/265M [00:01<00:00, 185MB/s]\r",
      "Downloading pytorch_model.bin:  91%|█████████ | 241M/265M [00:01<00:00, 187MB/s]\r",
      "Downloading pytorch_model.bin:  99%|█████████▊| 262M/265M [00:01<00:00, 188MB/s]\r",
      "Downloading pytorch_model.bin: 100%|██████████| 265M/265M [00:01<00:00, 179MB/s]\n",
      "\r",
      "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]\r",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 4.61kB/s]\n",
      "\r",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]\r",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 33.4kB/s]\n",
      "\r",
      "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\r",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.52MB/s]\r",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.43MB/s]\n",
      "\r",
      "Downloading tokenizer_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]\r",
      "Downloading tokenizer_config.json: 100%|██████████| 505/505 [00:00<00:00, 149kB/s]\n",
      "\r",
      "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\r",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.68MB/s]\n",
      "\r",
      "Downloading modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]\r",
      "Downloading modules.json: 100%|██████████| 229/229 [00:00<00:00, 71.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2dKuaVRbVan"
   },
   "source": [
    "## Task 4: Generate or Load the Embeddings\n",
    "\n",
    "In this task, you’ll either create or load the embeddings for all the paper summaries in the dataset.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Do one of the following:\n",
    "\n",
    "To create the embeddings, use the encode() method to generate embedding vectors for the paper summaries using the DistilBERT model. Remember to \n",
    "save the embeddings using pickle.dump().\n",
    "\n",
    "To load the embeddings that have been provided, open the default_embeddings.pickle file and load it to an embeddings object using the pickle.load()\n",
    "method.\n",
    "\n",
    "To load the embeddings that were generated in an earlier session, open the new_embeddings.pickle file and load it to an embeddings object using the\n",
    "pickle.load() method.\n",
    "    \n",
    "2. Create the variable length, which is the number of embeddings in the index.\n",
    "\n",
    "3. Print the shape of any one embedding.\n",
    "\n",
    "N.B : The code given in this task for generating embeddings will create embeddings for the first 2000 records of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bf5553fb54764284906ee83adf69a558",
      "92f959221add41669661b53b9ae66800",
      "77f4a144135145dfa4fe20d478a0967f",
      "4ffaad03b2f444fa930ef980a1c107f3",
      "d45d032e07494aec84f4c19b351e8378",
      "bcdb66968f5e4dbe8c75641b4a65193c",
      "0239d3187339442295198532950f9982",
      "bfd32f3063a34f4ea2859d8f40b86ca6",
      "7bc4b05546b541b8887a5a7bd1e66b69",
      "5760cebfb46440f6a9fada077a4d512c",
      "d80941e0bb7c406d8898300ba0267f36"
     ]
    },
    "id": "Y_GS0_CWVyR1",
    "outputId": "a5059d7d-5d85-42f2-b39c-dd4da1d7272e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/63 [00:00<?, ?it/s]\r",
      "Batches:   2%|▏         | 1/63 [00:06<06:38,  6.43s/it]\r",
      "Batches:   3%|▎         | 2/63 [00:12<06:16,  6.16s/it]\r",
      "Batches:   5%|▍         | 3/63 [00:18<06:11,  6.20s/it]\r",
      "Batches:   6%|▋         | 4/63 [00:27<07:17,  7.41s/it]\r",
      "Batches:   8%|▊         | 5/63 [00:34<06:42,  6.95s/it]\r",
      "Batches:  10%|▉         | 6/63 [00:40<06:17,  6.62s/it]\r",
      "Batches:  11%|█         | 7/63 [00:46<06:00,  6.43s/it]\r",
      "Batches:  13%|█▎        | 8/63 [00:54<06:26,  7.02s/it]\r",
      "Batches:  14%|█▍        | 9/63 [01:01<06:22,  7.09s/it]\r",
      "Batches:  16%|█▌        | 10/63 [01:07<05:58,  6.76s/it]\r",
      "Batches:  17%|█▋        | 11/63 [01:13<05:36,  6.48s/it]\r",
      "Batches:  19%|█▉        | 12/63 [01:19<05:27,  6.43s/it]\r",
      "Batches:  21%|██        | 13/63 [01:28<06:03,  7.27s/it]\r",
      "Batches:  22%|██▏       | 14/63 [01:35<05:40,  6.94s/it]\r",
      "Batches:  24%|██▍       | 15/63 [01:41<05:19,  6.65s/it]\r",
      "Batches:  25%|██▌       | 16/63 [01:47<05:03,  6.46s/it]\r",
      "Batches:  27%|██▋       | 17/63 [01:56<05:30,  7.18s/it]\r",
      "Batches:  29%|██▊       | 18/63 [02:02<05:10,  6.91s/it]\r",
      "Batches:  30%|███       | 19/63 [02:08<04:52,  6.65s/it]\r",
      "Batches:  32%|███▏      | 20/63 [02:14<04:40,  6.53s/it]\r",
      "Batches:  33%|███▎      | 21/63 [02:20<04:31,  6.47s/it]\r",
      "Batches:  35%|███▍      | 22/63 [02:29<04:54,  7.18s/it]\r",
      "Batches:  37%|███▋      | 23/63 [02:35<04:33,  6.83s/it]\r",
      "Batches:  38%|███▊      | 24/63 [02:41<04:16,  6.58s/it]\r",
      "Batches:  40%|███▉      | 25/63 [02:47<04:04,  6.43s/it]\r",
      "Batches:  41%|████▏     | 26/63 [02:57<04:31,  7.33s/it]\r",
      "Batches:  43%|████▎     | 27/63 [03:03<04:09,  6.92s/it]\r",
      "Batches:  44%|████▍     | 28/63 [03:09<03:53,  6.67s/it]\r",
      "Batches:  46%|████▌     | 29/63 [03:15<03:40,  6.49s/it]\r",
      "Batches:  48%|████▊     | 30/63 [03:22<03:39,  6.66s/it]\r",
      "Batches:  49%|████▉     | 31/63 [03:30<03:48,  7.13s/it]\r",
      "Batches:  51%|█████     | 32/63 [03:36<03:30,  6.79s/it]\r",
      "Batches:  52%|█████▏    | 33/63 [03:42<03:17,  6.58s/it]\r",
      "Batches:  54%|█████▍    | 34/63 [03:48<03:05,  6.40s/it]\r",
      "Batches:  56%|█████▌    | 35/63 [03:58<03:24,  7.30s/it]\r",
      "Batches:  57%|█████▋    | 36/63 [04:04<03:07,  6.95s/it]\r",
      "Batches:  59%|█████▊    | 37/63 [04:10<02:53,  6.67s/it]\r",
      "Batches:  60%|██████    | 38/63 [04:16<02:41,  6.47s/it]\r",
      "Batches:  62%|██████▏   | 39/63 [04:24<02:47,  6.97s/it]\r",
      "Batches:  63%|██████▎   | 40/63 [04:31<02:40,  7.00s/it]\r",
      "Batches:  65%|██████▌   | 41/63 [04:37<02:27,  6.71s/it]\r",
      "Batches:  67%|██████▋   | 42/63 [04:43<02:17,  6.56s/it]\r",
      "Batches:  68%|██████▊   | 43/63 [04:50<02:09,  6.47s/it]\r",
      "Batches:  70%|██████▉   | 44/63 [04:58<02:17,  7.22s/it]\r",
      "Batches:  71%|███████▏  | 45/63 [05:05<02:04,  6.94s/it]\r",
      "Batches:  73%|███████▎  | 46/63 [05:11<01:53,  6.67s/it]\r",
      "Batches:  75%|███████▍  | 47/63 [05:17<01:44,  6.52s/it]\r",
      "Batches:  76%|███████▌  | 48/63 [05:26<01:49,  7.31s/it]\r",
      "Batches:  78%|███████▊  | 49/63 [05:32<01:37,  6.96s/it]\r",
      "Batches:  79%|███████▉  | 50/63 [05:38<01:26,  6.65s/it]\r",
      "Batches:  81%|████████  | 51/63 [05:44<01:17,  6.48s/it]\r",
      "Batches:  83%|████████▎ | 52/63 [05:51<01:12,  6.55s/it]\r",
      "Batches:  84%|████████▍ | 53/63 [06:00<01:11,  7.16s/it]\r",
      "Batches:  86%|████████▌ | 54/63 [06:06<01:01,  6.82s/it]\r",
      "Batches:  87%|████████▋ | 55/63 [06:12<00:52,  6.60s/it]\r",
      "Batches:  89%|████████▉ | 56/63 [06:18<00:44,  6.42s/it]\r",
      "Batches:  90%|█████████ | 57/63 [06:27<00:43,  7.32s/it]\r",
      "Batches:  92%|█████████▏| 58/63 [06:33<00:34,  6.93s/it]\r",
      "Batches:  94%|█████████▎| 59/63 [06:39<00:26,  6.67s/it]\r",
      "Batches:  95%|█████████▌| 60/63 [06:45<00:19,  6.47s/it]\r",
      "Batches:  97%|█████████▋| 61/63 [06:51<00:12,  6.17s/it]\r",
      "Batches:  98%|█████████▊| 62/63 [06:59<00:06,  6.70s/it]\r",
      "Batches: 100%|██████████| 63/63 [07:01<00:00,  5.26s/it]\r",
      "Batches: 100%|██████████| 63/63 [07:01<00:00,  6.68s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(df.summary.to_list()[:2000], show_progress_bar=True)\n",
    "with open('/usercode/new_embeddings.pickle', 'wb') as pkl:\n",
    "  pickle.dump(embeddings, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UbzDs5wZUrKW"
   },
   "outputs": [],
   "source": [
    "with open('/usercode/default_embeddings.pickle', 'rb') as pkl:\n",
    "  embeddings = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/usercode/new_embeddings.pickle', 'rb') as pkl:\n",
    "  embeddings = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = len(embeddings)\n",
    "length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the one embedding:  (768,)\n",
      "(2000, 768)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the one embedding: ', embeddings[0].shape)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMYHGC_0RDw0"
   },
   "source": [
    "## Task 5: Data Preparation and Helper Methods\n",
    "\n",
    "In this task, you’ll prepare the dataset by encoding the paper IDs as integers. You’ll then write a helper method for returning a list of the \n",
    "required dataset information, given a list of IDs.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. The Faiss library requires integer IDs for the items present in the DataFrame. The dataset, on the other hand, has string IDs for each item.\n",
    "Use the fit_transform() method of the sklearn.preprocessing.LabelEncoder class to convert the string item IDs to integer values.\n",
    "\n",
    "2. Write a function named id2info(), which returns a list of column values for papers specified by their IDs. This method accepts the \n",
    "following parameters as input:\n",
    "\n",
    "df: This is the DataFrame in which the data is contained.\n",
    "\n",
    "I: This is a list of IDs of the papers for which the information is required.\n",
    "\n",
    "column: This is the column of the DataFrame where the required information is stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3EtlXeHpJULZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>36693</td>\n",
       "      <td>2</td>\n",
       "      <td>We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.</td>\n",
       "      <td>Dual Recurrent Attention Units for Visual Question Answering</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>18198</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.</td>\n",
       "      <td>Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day     id  month  \\\n",
       "0    1  36693      2   \n",
       "1   12  18198      3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              summary  \\\n",
       "0  We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.   \n",
       "1                                                                                                                                                                                      Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.   \n",
       "\n",
       "                                                                                      title  \\\n",
       "0                              Dual Recurrent Attention Units for Visual Question Answering   \n",
       "1  Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks   \n",
       "\n",
       "   year  \n",
       "0  2018  \n",
       "1  2016  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df['id'] = le.fit_transform(df['id'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "1DSJJsAUWT5K",
    "outputId": "50ccdd90-37ab-4751-9356-d1ab6483a115"
   },
   "outputs": [],
   "source": [
    "def id2info(df, I, column):\n",
    "    return [list(df[df.id == idx][column]) for idx in I]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGV4Je1EVyR_"
   },
   "source": [
    "## Task 6: Set up the Index\n",
    "\n",
    "In this task, you will set up the search index using the Faiss library.\n",
    "\n",
    "To complete this task, perform the following steps:\n",
    "\n",
    "1. Convert the embeddings to NumPy arrays of the float32 data type.\n",
    "\n",
    "2. Initialize the index using the IndexFlatL2() method from the Faiss library. This method will take the length of the embeddings as input. This \n",
    "index will return search results based on the k-nearest-neighbors algorithm through a brute-force search with L2 (Euclidean) distances.\n",
    "    \n",
    "3. Use the IndexIDMap() from the Faiss library to create an index map that encapsulates the initialized index and provides a mapping between IDs and \n",
    "the embedding vectors when adding and searching. This method will take the index that was created in the previous step as a parameter.\n",
    "\n",
    "4. Use the add_with_ids() method to add the embeddings and their IDs to the index map. This method will take the following parameters:\n",
    "\n",
    "embeddings: An array of embedding vectors\n",
    "\n",
    "xids: The IDs corresponding to the embedding vectors\n",
    "\n",
    "5. Print the number of embeddings in the index map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kkUDtwHVyR_",
    "outputId": "c193e0a3-379a-4067-ecbd-1fb706e48ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in the Faiss index:  2000\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index = faiss.IndexIDMap(index)\n",
    "index.add_with_ids(embeddings, df['id'][:length])\n",
    "\n",
    "print(\"Number of embeddings in the Faiss index: \", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l1G20oPMhza"
   },
   "source": [
    "## Task 7: Search with a Summary\n",
    "\n",
    "In this section, you’ll search the index with a summary from the dataset. To complete this task, perform the following steps:\n",
    "\n",
    "1. Print the summary that will be used to search.\n",
    "\n",
    "2. Get the 10 nearest neighbors by searching the index map. Search the index by using the index.search() method.\n",
    "\n",
    "This method accepts the following arguments:\n",
    "\n",
    "vector: These are the embeddings of the summary that will be used to search.\n",
    "\n",
    "k: This is the number of neighbors the model will return.\n",
    "    \n",
    "The method will return the following outputs:\n",
    "\n",
    "D: A list of L2 (Euclidean) distances of the results.\n",
    "\n",
    "I: The IDs of the results.\n",
    "    \n",
    "3. Display the distances, IDs, titles, and summaries of the returned results as a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "eEeJt7lYVySN",
    "outputId": "940e5767-48df-4b66-db7e-0c001810f1e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary    In this paper we study the application of convolutional neural networks for\\njointly detecting objects depicted in still images and estimating their 3D\\npose. We identify different feature representations of oriented objects, and\\nenergies that lead a network to learn this representations. The choice of the\\nrepresentation is crucial since the pose of an object has a natural, continuous\\nstructure while its category is a discrete variable. We evaluate the different\\napproaches on the joint object detection and pose estimation task of the\\nPascal3D+ benchmark using Average Viewpoint Precision. We show that a\\nclassification approach on discretized viewpoints achieves state-of-the-art\\nperformance for joint object detection and pose estimation, and significantly\\noutperforms existing baselines on this benchmark.\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         12964\n",
       "Name: 1337, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1337, [3, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary    We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      36693\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, [3, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "BSuRcH85VySQ",
    "outputId": "c430d590-83dc-40a6-d330-b4c5901ddb6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L2 distance</th>\n",
       "      <th>ML paper IDs</th>\n",
       "      <th>ML paper titles</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12964</td>\n",
       "      <td>[Convolutional Neural Networks for joint object detection and pose\\n  estimation: A comparative study]</td>\n",
       "      <td>[In this paper we study the application of convolutional neural networks for\\njointly detecting objects depicted in still images and estimating their 3D\\npose. We identify different feature representations of oriented objects, and\\nenergies that lead a network to learn this representations. The choice of the\\nrepresentation is crucial since the pose of an object has a natural, continuous\\nstructure while its category is a discrete variable. We evaluate the different\\napproaches on the joint object detection and pose estimation task of the\\nPascal3D+ benchmark using Average Viewpoint Precision. We show that a\\nclassification approach on discretized viewpoints achieves state-of-the-art\\nperformance for joint object detection and pose estimation, and significantly\\noutperforms existing baselines on this benchmark.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.530529</td>\n",
       "      <td>11503</td>\n",
       "      <td>[Deep Metric Learning for Practical Person Re-Identification]</td>\n",
       "      <td>[Various hand-crafted features and metric learning methods prevail in the\\nfield of person re-identification. Compared to these methods, this paper\\nproposes a more general way that can learn a similarity metric from image\\npixels directly. By using a \"siamese\" deep neural network, the proposed method\\ncan jointly learn the color feature, texture feature and metric in a unified\\nframework. The network has a symmetry structure with two sub-networks which are\\nconnected by Cosine function. To deal with the big variations of person images,\\nbinomial deviance is used to evaluate the cost between similarities and labels,\\nwhich is proved to be robust to outliers.\\n  Compared to existing researches, a more practical setting is studied in the\\nexperiments that is training and test on different datasets (cross dataset\\nperson re-identification). Both in \"intra dataset\" and \"cross dataset\"\\nsettings, the superiorities of the proposed method are illustrated on VIPeR and\\nPRID.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.805099</td>\n",
       "      <td>11377</td>\n",
       "      <td>[Cortical spatio-temporal dimensionality reduction for visual grouping]</td>\n",
       "      <td>[The visual systems of many mammals, including humans, is able to integrate\\nthe geometric information of visual stimuli and to perform cognitive tasks\\nalready at the first stages of the cortical processing. This is thought to be\\nthe result of a combination of mechanisms, which include feature extraction at\\nsingle cell level and geometric processing by means of cells connectivity. We\\npresent a geometric model of such connectivities in the space of detected\\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\\nused to obtain low-level object segmentation. The main idea is that of defining\\na spectral clustering procedure with anisotropic affinities over datasets\\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\\nNeural plausibility of the proposed arguments will be discussed.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.032898</td>\n",
       "      <td>11142</td>\n",
       "      <td>[Heterogeneous Multi-task Learning for Human Pose Estimation with Deep\\n  Convolutional Neural Network]</td>\n",
       "      <td>[We propose an heterogeneous multi-task learning framework for human pose\\nestimation from monocular image with deep convolutional neural network. In\\nparticular, we simultaneously learn a pose-joint regressor and a sliding-window\\nbody-part detector in a deep network architecture. We show that including the\\nbody-part detection task helps to regularize the network, directing it to\\nconverge to a good solution. We report competitive and state-of-art results on\\nseveral data sets. We also empirically show that the learned neurons in the\\nmiddle layer of our network are tuned to localized body parts.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.937653</td>\n",
       "      <td>30623</td>\n",
       "      <td>[Neural Expectation Maximization]</td>\n",
       "      <td>[Many real world tasks such as reasoning and physical interaction require\\nidentification and manipulation of conceptual entities. A first step towards\\nsolving these tasks is the automated discovery of distributed symbol-like\\nrepresentations. In this paper, we explicitly formalize this problem as\\ninference in a spatial mixture model where each component is parametrized by a\\nneural network. Based on the Expectation Maximization framework we then derive\\na differentiable clustering method that simultaneously learns how to group and\\nrepresent individual entities. We evaluate our method on the (sequential)\\nperceptual grouping task and find that it is able to accurately recover the\\nconstituent objects. We demonstrate that the learned representations are useful\\nfor next-step prediction.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.530083</td>\n",
       "      <td>13876</td>\n",
       "      <td>[Pixel-wise Deep Learning for Contour Detection]</td>\n",
       "      <td>[We address the problem of contour detection via per-pixel classifications of\\nedge point. To facilitate the process, the proposed approach leverages with\\nDenseNet, an efficient implementation of multiscale convolutional neural\\nnetworks (CNNs), to extract an informative feature vector for each pixel and\\nuses an SVM classifier to accomplish contour detection. In the experiment of\\ncontour detection, we look into the effectiveness of combining per-pixel\\nfeatures from different CNN layers and verify their performance on BSDS500.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>73.332405</td>\n",
       "      <td>18371</td>\n",
       "      <td>[Sparse Activity and Sparse Connectivity in Supervised Learning]</td>\n",
       "      <td>[Sparseness is a useful regularizer for learning in a wide range of\\napplications, in particular in neural networks. This paper proposes a model\\ntargeted at classification tasks, where sparse activity and sparse connectivity\\nare used to enhance classification capabilities. The tool for achieving this is\\na sparseness-enforcing projection operator which finds the closest vector with\\na pre-defined sparseness for any given vector. In the theoretical part of this\\npaper, a comprehensive theory for such a projection is developed. In\\nconclusion, it is shown that the projection is differentiable almost everywhere\\nand can thus be implemented as a smooth neuronal transfer function. The entire\\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\\non the MNIST database of handwritten digits show that classification\\nperformance can be boosted by sparse activity or sparse connectivity. With a\\ncombination of both, performance can be significantly better compared to\\nclassical non-sparse approaches.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73.559280</td>\n",
       "      <td>10182</td>\n",
       "      <td>[Deeply Coupled Auto-encoder Networks for Cross-view Classification]</td>\n",
       "      <td>[The comparison of heterogeneous samples extensively exists in many\\napplications, especially in the task of image classification. In this paper, we\\npropose a simple but effective coupled neural network, called Deeply Coupled\\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\\ncoupled with each other in every corresponding layers. In DCAN, each deep\\nstructure is developed via stacking multiple discriminative coupled\\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\\nconsisting of intra-class compactness and inter-class penalty. This single\\nlayer component makes our model simultaneously preserve the local consistency\\nand enhance its discriminative capability. With increasing number of layers,\\nthe coupled networks can gradually narrow the gap between the two views.\\nExtensive experiments on cross-view image classification tasks demonstrate the\\nsuperiority of our method over state-of-the-art methods.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73.715988</td>\n",
       "      <td>21064</td>\n",
       "      <td>[Crafting a multi-task CNN for viewpoint estimation]</td>\n",
       "      <td>[Convolutional Neural Networks (CNNs) were recently shown to provide\\nstate-of-the-art results for object category viewpoint estimation. However\\ndifferent ways of formulating this problem have been proposed and the competing\\napproaches have been explored with very different design choices. This paper\\npresents a comparison of these approaches in a unified setting as well as a\\ndetailed analysis of the key factors that impact performance. Followingly, we\\npresent a new joint training method with the detection task and demonstrate its\\nbenefit. We also highlight the superiority of classification approaches over\\nregression approaches, quantify the benefits of deeper architectures and\\nextended training data, and demonstrate that synthetic data is beneficial even\\nwhen using ImageNet training data. By combining all these elements, we\\ndemonstrate an improvement of approximately 5% mAVP over previous\\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\\nchallenging 24 view classification task we improve the results from 31.1% to\\n36.1% mAVP.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>73.727539</td>\n",
       "      <td>18667</td>\n",
       "      <td>[Deep Aesthetic Quality Assessment with Semantic Information]</td>\n",
       "      <td>[Human beings often assess the aesthetic quality of an image coupled with the\\nidentification of the image's semantic content. This paper addresses the\\ncorrelation issue between automatic aesthetic quality assessment and semantic\\nrecognition. We cast the assessment problem as the main task among a multi-task\\ndeep model, and argue that semantic recognition task offers the key to address\\nthis problem. Based on convolutional neural networks, we employ a single and\\nsimple multi-task framework to efficiently utilize the supervision of aesthetic\\nand semantic labels. A correlation item between these two tasks is further\\nintroduced to the framework by incorporating the inter-task relationship\\nlearning. This item not only provides some useful insight about the correlation\\nbut also improves assessment accuracy of the aesthetic task. Particularly, an\\neffective strategy is developed to keep a balance between the two tasks, which\\nfacilitates to optimize the parameters of the framework. Extensive experiments\\non the challenging AVA dataset and Photo.net dataset validate the importance of\\nsemantic recognition in aesthetic quality assessment, and demonstrate that\\nmulti-task deep models can discover an effective aesthetic representation to\\nachieve state-of-the-art results.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L2 distance  ML paper IDs  \\\n",
       "0     0.000000         12964   \n",
       "1    61.530529         11503   \n",
       "2    65.805099         11377   \n",
       "3    67.032898         11142   \n",
       "4    69.937653         30623   \n",
       "5    70.530083         13876   \n",
       "6    73.332405         18371   \n",
       "7    73.559280         10182   \n",
       "8    73.715988         21064   \n",
       "9    73.727539         18667   \n",
       "\n",
       "                                                                                           ML paper titles  \\\n",
       "0   [Convolutional Neural Networks for joint object detection and pose\\n  estimation: A comparative study]   \n",
       "1                                            [Deep Metric Learning for Practical Person Re-Identification]   \n",
       "2                                  [Cortical spatio-temporal dimensionality reduction for visual grouping]   \n",
       "3  [Heterogeneous Multi-task Learning for Human Pose Estimation with Deep\\n  Convolutional Neural Network]   \n",
       "4                                                                        [Neural Expectation Maximization]   \n",
       "5                                                         [Pixel-wise Deep Learning for Contour Detection]   \n",
       "6                                         [Sparse Activity and Sparse Connectivity in Supervised Learning]   \n",
       "7                                     [Deeply Coupled Auto-encoder Networks for Cross-view Classification]   \n",
       "8                                                     [Crafting a multi-task CNN for viewpoint estimation]   \n",
       "9                                            [Deep Aesthetic Quality Assessment with Semantic Information]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Summaries  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [In this paper we study the application of convolutional neural networks for\\njointly detecting objects depicted in still images and estimating their 3D\\npose. We identify different feature representations of oriented objects, and\\nenergies that lead a network to learn this representations. The choice of the\\nrepresentation is crucial since the pose of an object has a natural, continuous\\nstructure while its category is a discrete variable. We evaluate the different\\napproaches on the joint object detection and pose estimation task of the\\nPascal3D+ benchmark using Average Viewpoint Precision. We show that a\\nclassification approach on discretized viewpoints achieves state-of-the-art\\nperformance for joint object detection and pose estimation, and significantly\\noutperforms existing baselines on this benchmark.]  \n",
       "1                                                                                                                                                                                                                                                                                                                       [Various hand-crafted features and metric learning methods prevail in the\\nfield of person re-identification. Compared to these methods, this paper\\nproposes a more general way that can learn a similarity metric from image\\npixels directly. By using a \"siamese\" deep neural network, the proposed method\\ncan jointly learn the color feature, texture feature and metric in a unified\\nframework. The network has a symmetry structure with two sub-networks which are\\nconnected by Cosine function. To deal with the big variations of person images,\\nbinomial deviance is used to evaluate the cost between similarities and labels,\\nwhich is proved to be robust to outliers.\\n  Compared to existing researches, a more practical setting is studied in the\\nexperiments that is training and test on different datasets (cross dataset\\nperson re-identification). Both in \"intra dataset\" and \"cross dataset\"\\nsettings, the superiorities of the proposed method are illustrated on VIPeR and\\nPRID.]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                             [The visual systems of many mammals, including humans, is able to integrate\\nthe geometric information of visual stimuli and to perform cognitive tasks\\nalready at the first stages of the cortical processing. This is thought to be\\nthe result of a combination of mechanisms, which include feature extraction at\\nsingle cell level and geometric processing by means of cells connectivity. We\\npresent a geometric model of such connectivities in the space of detected\\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\\nused to obtain low-level object segmentation. The main idea is that of defining\\na spectral clustering procedure with anisotropic affinities over datasets\\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\\nNeural plausibility of the proposed arguments will be discussed.]  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [We propose an heterogeneous multi-task learning framework for human pose\\nestimation from monocular image with deep convolutional neural network. In\\nparticular, we simultaneously learn a pose-joint regressor and a sliding-window\\nbody-part detector in a deep network architecture. We show that including the\\nbody-part detection task helps to regularize the network, directing it to\\nconverge to a good solution. We report competitive and state-of-art results on\\nseveral data sets. We also empirically show that the learned neurons in the\\nmiddle layer of our network are tuned to localized body parts.]  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Many real world tasks such as reasoning and physical interaction require\\nidentification and manipulation of conceptual entities. A first step towards\\nsolving these tasks is the automated discovery of distributed symbol-like\\nrepresentations. In this paper, we explicitly formalize this problem as\\ninference in a spatial mixture model where each component is parametrized by a\\nneural network. Based on the Expectation Maximization framework we then derive\\na differentiable clustering method that simultaneously learns how to group and\\nrepresent individual entities. We evaluate our method on the (sequential)\\nperceptual grouping task and find that it is able to accurately recover the\\nconstituent objects. We demonstrate that the learned representations are useful\\nfor next-step prediction.]  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [We address the problem of contour detection via per-pixel classifications of\\nedge point. To facilitate the process, the proposed approach leverages with\\nDenseNet, an efficient implementation of multiscale convolutional neural\\nnetworks (CNNs), to extract an informative feature vector for each pixel and\\nuses an SVM classifier to accomplish contour detection. In the experiment of\\ncontour detection, we look into the effectiveness of combining per-pixel\\nfeatures from different CNN layers and verify their performance on BSDS500.]  \n",
       "6                                                                                                                                                                                                                                                                    [Sparseness is a useful regularizer for learning in a wide range of\\napplications, in particular in neural networks. This paper proposes a model\\ntargeted at classification tasks, where sparse activity and sparse connectivity\\nare used to enhance classification capabilities. The tool for achieving this is\\na sparseness-enforcing projection operator which finds the closest vector with\\na pre-defined sparseness for any given vector. In the theoretical part of this\\npaper, a comprehensive theory for such a projection is developed. In\\nconclusion, it is shown that the projection is differentiable almost everywhere\\nand can thus be implemented as a smooth neuronal transfer function. The entire\\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\\non the MNIST database of handwritten digits show that classification\\nperformance can be boosted by sparse activity or sparse connectivity. With a\\ncombination of both, performance can be significantly better compared to\\nclassical non-sparse approaches.]  \n",
       "7                                                                                                                                                                                                                                                                                                                                 [The comparison of heterogeneous samples extensively exists in many\\napplications, especially in the task of image classification. In this paper, we\\npropose a simple but effective coupled neural network, called Deeply Coupled\\nAutoencoder Networks (DCAN), which seeks to build two deep neural networks,\\ncoupled with each other in every corresponding layers. In DCAN, each deep\\nstructure is developed via stacking multiple discriminative coupled\\nauto-encoders, a denoising auto-encoder trained with maximum margin criterion\\nconsisting of intra-class compactness and inter-class penalty. This single\\nlayer component makes our model simultaneously preserve the local consistency\\nand enhance its discriminative capability. With increasing number of layers,\\nthe coupled networks can gradually narrow the gap between the two views.\\nExtensive experiments on cross-view image classification tasks demonstrate the\\nsuperiority of our method over state-of-the-art methods.]  \n",
       "8                                                                                                                                                                                                                 [Convolutional Neural Networks (CNNs) were recently shown to provide\\nstate-of-the-art results for object category viewpoint estimation. However\\ndifferent ways of formulating this problem have been proposed and the competing\\napproaches have been explored with very different design choices. This paper\\npresents a comparison of these approaches in a unified setting as well as a\\ndetailed analysis of the key factors that impact performance. Followingly, we\\npresent a new joint training method with the detection task and demonstrate its\\nbenefit. We also highlight the superiority of classification approaches over\\nregression approaches, quantify the benefits of deeper architectures and\\nextended training data, and demonstrate that synthetic data is beneficial even\\nwhen using ImageNet training data. By combining all these elements, we\\ndemonstrate an improvement of approximately 5% mAVP over previous\\nstate-of-the-art results on the Pascal3D+ dataset. In particular for their most\\nchallenging 24 view classification task we improve the results from 31.1% to\\n36.1% mAVP.]  \n",
       "9  [Human beings often assess the aesthetic quality of an image coupled with the\\nidentification of the image's semantic content. This paper addresses the\\ncorrelation issue between automatic aesthetic quality assessment and semantic\\nrecognition. We cast the assessment problem as the main task among a multi-task\\ndeep model, and argue that semantic recognition task offers the key to address\\nthis problem. Based on convolutional neural networks, we employ a single and\\nsimple multi-task framework to efficiently utilize the supervision of aesthetic\\nand semantic labels. A correlation item between these two tasks is further\\nintroduced to the framework by incorporating the inter-task relationship\\nlearning. This item not only provides some useful insight about the correlation\\nbut also improves assessment accuracy of the aesthetic task. Particularly, an\\neffective strategy is developed to keep a balance between the two tasks, which\\nfacilitates to optimize the parameters of the framework. Extensive experiments\\non the challenging AVA dataset and Photo.net dataset validate the importance of\\nsemantic recognition in aesthetic quality assessment, and demonstrate that\\nmulti-task deep models can discover an effective aesthetic representation to\\nachieve state-of-the-art results.]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I = index.search(np.array([embeddings[1337]]), k=10)\n",
    "pd.DataFrame({'L2 distance': D.flatten().tolist(), 'ML paper IDs': I.flatten().tolist(), 'ML paper titles': id2info(df, I.flatten(), 'title'), 'Summaries': id2info(df, I.flatten(), 'summary')}).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFKvRb4QY-DL"
   },
   "source": [
    "## Task 8: Search with a Prompt\n",
    "\n",
    "In this task, you’ll search the dataset using a prompt. To complete this task, perform the following steps:\n",
    "\n",
    "1. Create the prompt and assign it to a string variable named user_query.\n",
    "    \n",
    "2. Encode the prompt using the encode() method of the selected. This method accepts the prompt as a list.\n",
    "    \n",
    "3. Search the index using the generated embeddings, and display the distances, IDs, titles, and summaries in the results as a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iDhftkrhX99T"
   },
   "outputs": [],
   "source": [
    "\n",
    "user_query = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "6AFhbGnWZpWN",
    "outputId": "18385c17-a0e2-4ed6-8861-155c996d51e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L2 distances</th>\n",
       "      <th>ML paper IDs</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398.835114</td>\n",
       "      <td>26890</td>\n",
       "      <td>[Abstract Syntax Networks for Code Generation and Semantic Parsing]</td>\n",
       "      <td>[Tasks like code generation and semantic parsing require mapping unstructured\\n(or partially structured) inputs to well-formed, executable outputs. We\\nintroduce abstract syntax networks, a modeling framework for these problems.\\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\\na decoder with a dynamically-determined modular structure paralleling the\\nstructure of the output tree. On the benchmark Hearthstone dataset for code\\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\\nno task-specific engineering.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401.845093</td>\n",
       "      <td>30018</td>\n",
       "      <td>[Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\\n  Functions in Quasi-Recurrent Neural Networks]</td>\n",
       "      <td>[In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\\nunbounded positive and negative image, can be used as a drop-in replacement for\\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\\nprone to the vanishing gradient problem, they are noise robust, and they induce\\nsparse activations.\\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\\nlanguage modeling. Additionally, we evaluate on character-level language\\nmodeling, showing that we are able to stack up to eight QRNN layers with\\nDReLUs, thus making it possible to improve the current state-of-the-art in\\ncharacter-level language modeling over shallow architectures based on LSTMs.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>403.243622</td>\n",
       "      <td>7184</td>\n",
       "      <td>[KSU KDD: Word Sense Induction by Clustering in Topic Space]</td>\n",
       "      <td>[We describe our language-independent unsupervised word sense induction\\nsystem. This system only uses topic features to cluster different word senses\\nin their global context topic space. Using unlabeled data, this system trains a\\nlatent Dirichlet allocation (LDA) topic model then uses it to infer the topics\\ndistribution of the test instances. By clustering these topics distributions in\\ntheir topic space we cluster them into different senses. Our hypothesis is that\\ncloseness in topic space reflects similarity between different word senses.\\nThis system participated in SemEval-2 word sense induction and disambiguation\\ntask and achieved the second highest V-measure score among all other systems.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403.410858</td>\n",
       "      <td>28659</td>\n",
       "      <td>[Topic supervised non-negative matrix factorization]</td>\n",
       "      <td>[Topic models have been extensively used to organize and interpret the\\ncontents of large, unstructured corpora of text documents. Although topic\\nmodels often perform well on traditional training vs. test set evaluations, it\\nis often the case that the results of a topic model do not align with human\\ninterpretation. This interpretability fallacy is largely due to the\\nunsupervised nature of topic models, which prohibits any user guidance on the\\nresults of a model. In this paper, we introduce a semi-supervised method called\\ntopic supervised non-negative matrix factorization (TS-NMF) that enables the\\nuser to provide labeled example documents to promote the discovery of more\\nmeaningful semantic structure of a corpus. In this way, the results of TS-NMF\\nbetter match the intuition and desired labeling of the user. The core of TS-NMF\\nrelies on solving a non-convex optimization problem for which we derive an\\niterative algorithm that is shown to be monotonic and convergent to a local\\noptimum. We demonstrate the practical utility of TS-NMF on the Reuters and\\nPubMed corpora, and find that TS-NMF is especially useful for conceptual or\\nbroad topics, where topic key terms are not well understood. Although\\nidentifying an optimal latent structure for the data is not a primary objective\\nof the proposed approach, we find that TS-NMF achieves higher weighted Jaccard\\nsimilarity scores than the contemporary methods, (unsupervised) NMF and latent\\nDirichlet allocation, at supervision rates as low as 10% to 20%.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>404.912506</td>\n",
       "      <td>34786</td>\n",
       "      <td>[Don't Just Assume; Look and Answer: Overcoming Priors for Visual\\n  Question Answering]</td>\n",
       "      <td>[A number of studies have found that today's Visual Question Answering (VQA)\\nmodels are heavily driven by superficial correlations in the training data and\\nlack sufficient image grounding. To encourage development of models geared\\ntowards the latter, we propose a new setting for VQA where for every question\\ntype, train and test sets have different prior distributions of answers.\\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\\nrespectively). First, we evaluate several existing VQA models under this new\\nsetting and show that their performance degrades significantly compared to the\\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\\nAnswering model (GVQA) that contains inductive biases and restrictions in the\\narchitecture specifically designed to prevent the model from 'cheating' by\\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\\ndisentangles the recognition of visual concepts present in the image from the\\nidentification of plausible answer space for a given question, enabling the\\nmodel to more robustly generalize across different distributions of answers.\\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\\nseveral cases. GVQA offers strengths complementary to SAN when trained and\\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\\ntransparent and interpretable than existing VQA models.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>406.426178</td>\n",
       "      <td>30468</td>\n",
       "      <td>[Regularizing and Optimizing LSTM Language Models]</td>\n",
       "      <td>[Recurrent neural networks (RNNs), such as long short-term memory networks\\n(LSTMs), serve as a fundamental building block for many sequence learning\\ntasks, including machine translation, language modeling, and question\\nanswering. In this paper, we consider the specific problem of word-level\\nlanguage modeling and investigate strategies for regularizing and optimizing\\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\\nwherein the averaging trigger is determined using a non-monotonic condition as\\nopposed to being tuned by the user. Using these and other regularization\\nstrategies, we achieve state-of-the-art word level perplexities on two data\\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\\neffectiveness of a neural cache in conjunction with our proposed model, we\\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\\n52.0 on WikiText-2.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>410.384216</td>\n",
       "      <td>16734</td>\n",
       "      <td>[Learning the Dimensionality of Word Embeddings]</td>\n",
       "      <td>[We describe a method for learning word embeddings with data-dependent\\ndimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic\\nDimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of\\nMikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is\\nmade dynamic by employing techniques used by Cote &amp; Larochelle (2016) to define\\nan RBM with an infinite number of hidden units. We show qualitatively and\\nquantitatively that SD-SG and SD-CBOW are competitive with their\\nfixed-dimension counterparts while providing a distribution over embedding\\ndimensionalities, which offers a window into how semantics distribute across\\ndimensions.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>410.741394</td>\n",
       "      <td>12050</td>\n",
       "      <td>[HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\\n  Visual Recognition]</td>\n",
       "      <td>[In image classification, visual separability between different object\\ncategories is highly uneven, and some categories are more difficult to\\ndistinguish than others. Such difficult categories demand more dedicated\\nclassifiers. However, existing deep convolutional neural networks (CNN) are\\ntrained as flat N-way classifiers, and few efforts have been made to leverage\\nthe hierarchical structure of categories. In this paper, we introduce\\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\\nwhile distinguishing difficult classes using fine category classifiers. During\\nHD-CNN training, component-wise pretraining is followed by global finetuning\\nwith a multinomial logistic loss regularized by a coarse category consistency\\nterm. In addition, conditional executions of fine category classifiers and\\nlayer parameter compression make HD-CNNs scalable for large-scale visual\\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\\nCNNs by 2.65%, 3.1% and 1.1%, respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>414.264343</td>\n",
       "      <td>11912</td>\n",
       "      <td>[Taking into Account the Differences between Actively and Passively\\n  Acquired Data: The Case of Active Learning with Support Vector Machines for\\n  Imbalanced Datasets]</td>\n",
       "      <td>[Actively sampled data can have very different characteristics than passively\\nsampled data. Therefore, it's promising to investigate using different\\ninference procedures during AL than are used during passive learning (PL). This\\ngeneral idea is explored in detail for the focused case of AL with\\ncost-weighted SVMs for imbalanced data, a situation that arises for many HLT\\ntasks. The key idea behind the proposed InitPA method for addressing imbalance\\nis to base cost models during AL on an estimate of overall corpus imbalance\\ncomputed via a small unbiased sample rather than the imbalance in the labeled\\ntraining data, which is the leading method used during PL.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>414.365906</td>\n",
       "      <td>31758</td>\n",
       "      <td>[Self-Guiding Multimodal LSTM - when we do not have a perfect training\\n  dataset for image captioning]</td>\n",
       "      <td>[In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning\\nmodel is proposed to handle uncontrolled imbalanced real-world image-sentence\\ndataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165\\nimages and the original text descriptions uploaded by the users are utilized as\\nthe ground truth for training. Descriptions in FlickrNYC dataset vary\\ndramatically ranging from short term-descriptions to long\\nparagraph-descriptions and can describe any visual aspects, or even refer to\\nobjects that are not depicted. To deal with the imbalanced and noisy situation\\nand to fully explore the dataset itself, we propose a novel guiding textual\\nfeature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of\\nm-LSTM is based on the portion of data in which the image content and the\\ncorresponding descriptions are strongly bonded. Afterwards, during the training\\nof sg-LSTM on the rest training data, this guiding information serves as\\nadditional input to the network along with the image representations and the\\nground-truth descriptions. By integrating these input components into a\\nmultimodal block, we aim to form a training scheme with the textual information\\ntightly coupled with the image content. The experimental results demonstrate\\nthat the proposed sg-LSTM model outperforms the traditional state-of-the-art\\nmultimodal RNN captioning framework in successfully describing the key\\ncomponents of the input images.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L2 distances  ML paper IDs  \\\n",
       "0    398.835114         26890   \n",
       "1    401.845093         30018   \n",
       "2    403.243622          7184   \n",
       "3    403.410858         28659   \n",
       "4    404.912506         34786   \n",
       "5    406.426178         30468   \n",
       "6    410.384216         16734   \n",
       "7    410.741394         12050   \n",
       "8    414.264343         11912   \n",
       "9    414.365906         31758   \n",
       "\n",
       "                                                                                                                                                                       Titles  \\\n",
       "0                                                                                                         [Abstract Syntax Networks for Code Generation and Semantic Parsing]   \n",
       "1                                                   [Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\\n  Functions in Quasi-Recurrent Neural Networks]   \n",
       "2                                                                                                                [KSU KDD: Word Sense Induction by Clustering in Topic Space]   \n",
       "3                                                                                                                        [Topic supervised non-negative matrix factorization]   \n",
       "4                                                                                    [Don't Just Assume; Look and Answer: Overcoming Priors for Visual\\n  Question Answering]   \n",
       "5                                                                                                                          [Regularizing and Optimizing LSTM Language Models]   \n",
       "6                                                                                                                            [Learning the Dimensionality of Word Embeddings]   \n",
       "7                                                                              [HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale\\n  Visual Recognition]   \n",
       "8  [Taking into Account the Differences between Actively and Passively\\n  Acquired Data: The Case of Active Learning with Support Vector Machines for\\n  Imbalanced Datasets]   \n",
       "9                                                                     [Self-Guiding Multimodal LSTM - when we do not have a perfect training\\n  dataset for image captioning]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Summaries  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [Tasks like code generation and semantic parsing require mapping unstructured\\n(or partially structured) inputs to well-formed, executable outputs. We\\nintroduce abstract syntax networks, a modeling framework for these problems.\\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\\na decoder with a dynamically-determined modular structure paralleling the\\nstructure of the output tree. On the benchmark Hearthstone dataset for code\\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\\nno task-specific engineering.]  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     [In this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\\nunbounded positive and negative image, can be used as a drop-in replacement for\\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\\nprone to the vanishing gradient problem, they are noise robust, and they induce\\nsparse activations.\\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\\nlanguage modeling. Additionally, we evaluate on character-level language\\nmodeling, showing that we are able to stack up to eight QRNN layers with\\nDReLUs, thus making it possible to improve the current state-of-the-art in\\ncharacter-level language modeling over shallow architectures based on LSTMs.]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [We describe our language-independent unsupervised word sense induction\\nsystem. This system only uses topic features to cluster different word senses\\nin their global context topic space. Using unlabeled data, this system trains a\\nlatent Dirichlet allocation (LDA) topic model then uses it to infer the topics\\ndistribution of the test instances. By clustering these topics distributions in\\ntheir topic space we cluster them into different senses. Our hypothesis is that\\ncloseness in topic space reflects similarity between different word senses.\\nThis system participated in SemEval-2 word sense induction and disambiguation\\ntask and achieved the second highest V-measure score among all other systems.]  \n",
       "3                                                                                                                                                                                                                                           [Topic models have been extensively used to organize and interpret the\\ncontents of large, unstructured corpora of text documents. Although topic\\nmodels often perform well on traditional training vs. test set evaluations, it\\nis often the case that the results of a topic model do not align with human\\ninterpretation. This interpretability fallacy is largely due to the\\nunsupervised nature of topic models, which prohibits any user guidance on the\\nresults of a model. In this paper, we introduce a semi-supervised method called\\ntopic supervised non-negative matrix factorization (TS-NMF) that enables the\\nuser to provide labeled example documents to promote the discovery of more\\nmeaningful semantic structure of a corpus. In this way, the results of TS-NMF\\nbetter match the intuition and desired labeling of the user. The core of TS-NMF\\nrelies on solving a non-convex optimization problem for which we derive an\\niterative algorithm that is shown to be monotonic and convergent to a local\\noptimum. We demonstrate the practical utility of TS-NMF on the Reuters and\\nPubMed corpora, and find that TS-NMF is especially useful for conceptual or\\nbroad topics, where topic key terms are not well understood. Although\\nidentifying an optimal latent structure for the data is not a primary objective\\nof the proposed approach, we find that TS-NMF achieves higher weighted Jaccard\\nsimilarity scores than the contemporary methods, (unsupervised) NMF and latent\\nDirichlet allocation, at supervision rates as low as 10% to 20%.]  \n",
       "4  [A number of studies have found that today's Visual Question Answering (VQA)\\nmodels are heavily driven by superficial correlations in the training data and\\nlack sufficient image grounding. To encourage development of models geared\\ntowards the latter, we propose a new setting for VQA where for every question\\ntype, train and test sets have different prior distributions of answers.\\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\\nrespectively). First, we evaluate several existing VQA models under this new\\nsetting and show that their performance degrades significantly compared to the\\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\\nAnswering model (GVQA) that contains inductive biases and restrictions in the\\narchitecture specifically designed to prevent the model from 'cheating' by\\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\\ndisentangles the recognition of visual concepts present in the image from the\\nidentification of plausible answer space for a given question, enabling the\\nmodel to more robustly generalize across different distributions of answers.\\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\\nseveral cases. GVQA offers strengths complementary to SAN when trained and\\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\\ntransparent and interpretable than existing VQA models.]  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Recurrent neural networks (RNNs), such as long short-term memory networks\\n(LSTMs), serve as a fundamental building block for many sequence learning\\ntasks, including machine translation, language modeling, and question\\nanswering. In this paper, we consider the specific problem of word-level\\nlanguage modeling and investigate strategies for regularizing and optimizing\\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\\nwherein the averaging trigger is determined using a non-monotonic condition as\\nopposed to being tuned by the user. Using these and other regularization\\nstrategies, we achieve state-of-the-art word level perplexities on two data\\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\\neffectiveness of a neural cache in conjunction with our proposed model, we\\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\\n52.0 on WikiText-2.]  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [We describe a method for learning word embeddings with data-dependent\\ndimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic\\nDimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of\\nMikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is\\nmade dynamic by employing techniques used by Cote & Larochelle (2016) to define\\nan RBM with an infinite number of hidden units. We show qualitatively and\\nquantitatively that SD-SG and SD-CBOW are competitive with their\\nfixed-dimension counterparts while providing a distribution over embedding\\ndimensionalities, which offers a window into how semantics distribute across\\ndimensions.]  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [In image classification, visual separability between different object\\ncategories is highly uneven, and some categories are more difficult to\\ndistinguish than others. Such difficult categories demand more dedicated\\nclassifiers. However, existing deep convolutional neural networks (CNN) are\\ntrained as flat N-way classifiers, and few efforts have been made to leverage\\nthe hierarchical structure of categories. In this paper, we introduce\\nhierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category\\nhierarchy. An HD-CNN separates easy classes using a coarse category classifier\\nwhile distinguishing difficult classes using fine category classifiers. During\\nHD-CNN training, component-wise pretraining is followed by global finetuning\\nwith a multinomial logistic loss regularized by a coarse category consistency\\nterm. In addition, conditional executions of fine category classifiers and\\nlayer parameter compression make HD-CNNs scalable for large-scale visual\\nrecognition. We achieve state-of-the-art results on both CIFAR100 and\\nlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, we\\nbuild up three different HD-CNNs and they lower the top-1 error of the standard\\nCNNs by 2.65%, 3.1% and 1.1%, respectively.]  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Actively sampled data can have very different characteristics than passively\\nsampled data. Therefore, it's promising to investigate using different\\ninference procedures during AL than are used during passive learning (PL). This\\ngeneral idea is explored in detail for the focused case of AL with\\ncost-weighted SVMs for imbalanced data, a situation that arises for many HLT\\ntasks. The key idea behind the proposed InitPA method for addressing imbalance\\nis to base cost models during AL on an estimate of overall corpus imbalance\\ncomputed via a small unbiased sample rather than the imbalance in the labeled\\ntraining data, which is the leading method used during PL.]  \n",
       "9                                                                                                                                                                                                                                                                                                 [In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning\\nmodel is proposed to handle uncontrolled imbalanced real-world image-sentence\\ndataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165\\nimages and the original text descriptions uploaded by the users are utilized as\\nthe ground truth for training. Descriptions in FlickrNYC dataset vary\\ndramatically ranging from short term-descriptions to long\\nparagraph-descriptions and can describe any visual aspects, or even refer to\\nobjects that are not depicted. To deal with the imbalanced and noisy situation\\nand to fully explore the dataset itself, we propose a novel guiding textual\\nfeature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of\\nm-LSTM is based on the portion of data in which the image content and the\\ncorresponding descriptions are strongly bonded. Afterwards, during the training\\nof sg-LSTM on the rest training data, this guiding information serves as\\nadditional input to the network along with the image representations and the\\nground-truth descriptions. By integrating these input components into a\\nmultimodal block, we aim to form a training scheme with the textual information\\ntightly coupled with the image content. The experimental results demonstrate\\nthat the proposed sg-LSTM model outperforms the traditional state-of-the-art\\nmultimodal RNN captioning framework in successfully describing the key\\ncomponents of the input images.]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = model.encode(list(user_query))\n",
    "D, I = index.search(np.array([embed]).squeeze().astype(\"float32\"), k=10)\n",
    "\n",
    "results = {'L2 distances':D.flatten().tolist(), 'ML paper IDs':I.flatten().tolist(), \"Titles\": id2info(df, I.flatten(), 'title'), \"Summaries\": id2info(df, I.flatten(), 'summary')}\n",
    "\n",
    "pd.DataFrame(results).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpNrxUXbk0N7"
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "007d212316024b57adcac7871e445d10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5df03efae4f45dbb912cb8529154304",
       "IPY_MODEL_d798a2cb981e4bea896447a54eec09cc",
       "IPY_MODEL_0198a121ecfc40a7b6e9e6425186e4cd"
      ],
      "layout": "IPY_MODEL_59b33c71bdbe41e893c88c86b2dc704b"
     }
    },
    "0097d999ff37437eaefc09d25e51ef1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ed8f2bd24de42088f96181cd5253f72",
      "placeholder": "​",
      "style": "IPY_MODEL_fb366cdce58c478f8a7e69a336d7d70e",
      "value": " 345/345 [00:00&lt;00:00, 13.5kB/s]"
     }
    },
    "0198a121ecfc40a7b6e9e6425186e4cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bb104f572e445ffb01a7773fbd305e4",
      "placeholder": "​",
      "style": "IPY_MODEL_83741ea2737640078f21089c3dda2166",
      "value": " 190/190 [00:00&lt;00:00, 8.76kB/s]"
     }
    },
    "0239d3187339442295198532950f9982": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "03c465d2324245aea22fafe3753999c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08725b73d1064201b2a5ae59f71229f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09227dfc526c47929a938864fe2f7ee7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0be8b9bc5d4c417ab28216732e69bd57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dee9424ad704d17810409b0f5582fb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cab5e02948ca4895a51b911f6709b3e3",
      "placeholder": "​",
      "style": "IPY_MODEL_b29f27e7283d468b9ae6f38dd29b0c6d",
      "value": " 53.0/53.0 [00:00&lt;00:00, 2.31kB/s]"
     }
    },
    "100afbd6a4e841149873701151872b03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1076f43cda864471b019d9791950b011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10feddb6ec244d259c55144943e51987": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1249d6a245704760876d86b16c55aeb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1288bd5eda084e5c949937814888caca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a7ab4e3aa374789b9a30a973951733f",
       "IPY_MODEL_d38bba863f9e404a9b865c0a2c03311c",
       "IPY_MODEL_0dee9424ad704d17810409b0f5582fb4"
      ],
      "layout": "IPY_MODEL_6abeec7be8164db79447625ad5969002"
     }
    },
    "12f2b0efa8224cde834bd57882aea762": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14257bc929f7424092463b34d0fc4a61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "15cd689a923f423087fb6945f7ba22d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c465d2324245aea22fafe3753999c6",
      "placeholder": "​",
      "style": "IPY_MODEL_ffcf6844c579427da16fae54fddf20dc",
      "value": " 505/505 [00:00&lt;00:00, 27.3kB/s]"
     }
    },
    "1673f62c33924d6dbe159b665e092044": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1925808c688544bf9d071f588633b250": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "196c48db830f4a3bb2fcac35f673938d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1cc90111ab874b5cb4cbaee57f90129b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ed8f2bd24de42088f96181cd5253f72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f18b26b1fc84db0829bc2cbbc63ff10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e3677374c19471a84bb78c3dcddfca1",
       "IPY_MODEL_bc31a80accf844c3a6a2c89b673cef9b",
       "IPY_MODEL_2bab636ec40f44a8b7b8c7d4b1ae5a1f"
      ],
      "layout": "IPY_MODEL_672f6e70eef04b7dbf4ae5b78f132845"
     }
    },
    "1f598a0edd544bd4b39faac1de090b9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20072133987b4bcc88de1404d169bf9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "220b3167108c45aba1b72916293bb3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9700ea0ef3e4461a3c128d392838986",
      "placeholder": "​",
      "style": "IPY_MODEL_6af3912429794e65ab27e848d70d48c5",
      "value": "Downloading (…)7e0d5/.gitattributes: 100%"
     }
    },
    "24f46851a05d4f0ba7cedf9fe6b854c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85a1bbf88ecd481bb0f6b8ee019dd495",
      "placeholder": "​",
      "style": "IPY_MODEL_7cf2b46a8de34dc8b6aead85500ab6dd",
      "value": " 229/229 [00:00&lt;00:00, 13.4kB/s]"
     }
    },
    "28ee7786ee284adfb4b6cba3440a7a99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ae618c6e8084fde857a2d4b00fa5a86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bab636ec40f44a8b7b8c7d4b1ae5a1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20072133987b4bcc88de1404d169bf9d",
      "placeholder": "​",
      "style": "IPY_MODEL_1076f43cda864471b019d9791950b011",
      "value": " 122/122 [00:00&lt;00:00, 7.37kB/s]"
     }
    },
    "2e7ff5c089c840c298df34c9085e7288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a728a2ef35d447f890603d9c9f79f15d",
      "max": 229,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77368f4321704269ad4dc3c820a1a8f4",
      "value": 229
     }
    },
    "315f2c119f3d4eb792f4ce606290a53c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_429ccab3671546d3a62574332fa644c6",
       "IPY_MODEL_8cd3e20119fe4fa2bde2606e310b507c",
       "IPY_MODEL_5186fe054bd54b8f8badddb6caa9bd03"
      ],
      "layout": "IPY_MODEL_4744d955f20f4887aff2345b15347ae5"
     }
    },
    "33c3c1ebaa6f4cd6a352c9787f8fffe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3477b89bd24d4f6f94a57741ce0f744b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3709d5063ff242fda5150b1c8c6c4664": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37d30d9227094847b02c3f205904f1a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a377053c7da4c538145213bd5bf5652": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93fbf4a4c01847109b23b94c1561907e",
      "placeholder": "​",
      "style": "IPY_MODEL_bb3f9420a43c490985220119e79e0dbd",
      "value": " 232k/232k [00:00&lt;00:00, 327kB/s]"
     }
    },
    "3b5d0c5b41114272b23c5aaf6e2d5cf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c995640f16e4ff4aeede34f341728ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3db9e74d662c491da6e0314db32c754d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c995640f16e4ff4aeede34f341728ee",
      "max": 466081,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d8b13fbee904d49bc50778fee62afe7",
      "value": 466081
     }
    },
    "3e1885f62864416aa8ac36c8fca91277": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3fe0745adab24c6a82877ca471e5c32f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09227dfc526c47929a938864fe2f7ee7",
      "placeholder": "​",
      "style": "IPY_MODEL_1673f62c33924d6dbe159b665e092044",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "41869a9881ae42e7af2b7cef43a56f66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "429ccab3671546d3a62574332fa644c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c010bca4b46c426cb4d75734ee4d213e",
      "placeholder": "​",
      "style": "IPY_MODEL_1cc90111ab874b5cb4cbaee57f90129b",
      "value": "Downloading (…)0e5ca7e0d5/README.md: 100%"
     }
    },
    "44e9016964fe4c49ad77cd92a24c137b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdb19005a4e94f8a90c29260964304cc",
      "placeholder": "​",
      "style": "IPY_MODEL_33c3c1ebaa6f4cd6a352c9787f8fffe5",
      "value": " 265M/265M [00:01&lt;00:00, 251MB/s]"
     }
    },
    "4744d955f20f4887aff2345b15347ae5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a7b562563a849ed9ea2c7c159c305b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b15909c8b004279a389902115f89421": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b92df9fb7294c2da58fe1d2ddc610f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12f2b0efa8224cde834bd57882aea762",
      "max": 345,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_100afbd6a4e841149873701151872b03",
      "value": 345
     }
    },
    "4e212c53e4464720bbb8f2a5d22f7bf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ea10297a23249b7b1f4843e521e27de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ffaad03b2f444fa930ef980a1c107f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5760cebfb46440f6a9fada077a4d512c",
      "placeholder": "​",
      "style": "IPY_MODEL_d80941e0bb7c406d8898300ba0267f36",
      "value": " 1282/1282 [02:39&lt;00:00, 17.22it/s]"
     }
    },
    "5186fe054bd54b8f8badddb6caa9bd03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edb01c355cbc4e1eac31593ac2c8a555",
      "placeholder": "​",
      "style": "IPY_MODEL_d320f358e78c4d0a9bd0b5dc082a40b4",
      "value": " 4.01k/4.01k [00:00&lt;00:00, 176kB/s]"
     }
    },
    "53bd1728e57a400aac0c684bba95ccb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_220b3167108c45aba1b72916293bb3a3",
       "IPY_MODEL_4b92df9fb7294c2da58fe1d2ddc610f4",
       "IPY_MODEL_0097d999ff37437eaefc09d25e51ef1c"
      ],
      "layout": "IPY_MODEL_bfe7598cb6584390981f157f4d7db1c3"
     }
    },
    "54a15fdc18fb40029ebf9839225c40fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5760cebfb46440f6a9fada077a4d512c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57fbf00307494309900693460b93ee45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0716fb9e26f4673b23c912b0720e2ce",
       "IPY_MODEL_3db9e74d662c491da6e0314db32c754d",
       "IPY_MODEL_753b311297504d2e98c09c426710f3f7"
      ],
      "layout": "IPY_MODEL_ad6354fed78e4209a0eb7e5b0ef81fcc"
     }
    },
    "59b33c71bdbe41e893c88c86b2dc704b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b86e51a9ddd43478cb4ff0ca45d8724": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b716708dc7474328992c5b26697aa4e9",
       "IPY_MODEL_df7d9b5f58d6435187360a31e79b176a",
       "IPY_MODEL_44e9016964fe4c49ad77cd92a24c137b"
      ],
      "layout": "IPY_MODEL_2ae618c6e8084fde857a2d4b00fa5a86"
     }
    },
    "5d8b13fbee904d49bc50778fee62afe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "672f6e70eef04b7dbf4ae5b78f132845": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "696574fd545c4b55be32ea07e094b7d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a2abb6db22348bca9b9da53b0320e0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a7ab4e3aa374789b9a30a973951733f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1249d6a245704760876d86b16c55aeb2",
      "placeholder": "​",
      "style": "IPY_MODEL_a4d6f98130a142588ce66414279823ac",
      "value": "Downloading (…)nce_bert_config.json: 100%"
     }
    },
    "6abeec7be8164db79447625ad5969002": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6af3912429794e65ab27e848d70d48c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "732122923ebd4a1e9a59114a75212733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "734fc4060cc54b3eb64bfa32bd0975ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "753b311297504d2e98c09c426710f3f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08725b73d1064201b2a5ae59f71229f1",
      "placeholder": "​",
      "style": "IPY_MODEL_fcdefdf5882c44b995cadce70b4d1211",
      "value": " 466k/466k [00:00&lt;00:00, 502kB/s]"
     }
    },
    "75ce838fcfc745dca5fd9da268478d01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7d66b7cdf9624c56ae3b628399dc7c72",
       "IPY_MODEL_e044383b011f4863a05fb95770b33781",
       "IPY_MODEL_3a377053c7da4c538145213bd5bf5652"
      ],
      "layout": "IPY_MODEL_4e212c53e4464720bbb8f2a5d22f7bf8"
     }
    },
    "770a8586da4844dc84366444a8f65637": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77368f4321704269ad4dc3c820a1a8f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77f4a144135145dfa4fe20d478a0967f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfd32f3063a34f4ea2859d8f40b86ca6",
      "max": 1282,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bc4b05546b541b8887a5a7bd1e66b69",
      "value": 1282
     }
    },
    "780e3bdd45a5495592cc5fb44780f88c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa5ecefecef34cdba58ff4d4092ced56",
      "placeholder": "​",
      "style": "IPY_MODEL_732122923ebd4a1e9a59114a75212733",
      "value": " 112/112 [00:00&lt;00:00, 5.63kB/s]"
     }
    },
    "7a745e2bcba94554880c67aaa83c158b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bb104f572e445ffb01a7773fbd305e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bc4b05546b541b8887a5a7bd1e66b69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7cf2b46a8de34dc8b6aead85500ab6dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d5456b0ea6d4cbf95cd93e4db33485a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d66b7cdf9624c56ae3b628399dc7c72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a7b562563a849ed9ea2c7c159c305b1",
      "placeholder": "​",
      "style": "IPY_MODEL_97644a9c4fe148f0a1c62532d4a6fe5f",
      "value": "Downloading (…)0e5ca7e0d5/vocab.txt: 100%"
     }
    },
    "7ede626c3dde463b822a84e75a23face": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83741ea2737640078f21089c3dda2166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85a1bbf88ecd481bb0f6b8ee019dd495": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8658a29975974246af3df68e5e52198d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8795346c4376409d9b5126236b8ecee6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f56e474aa3734728b699c6c2ed6c709e",
      "max": 505,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef36ada088a24063b6ed058c2faf133a",
      "value": 505
     }
    },
    "896f716571074df9a0b4505da5a655ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89e895bdbb0d489592bbc23a19da5e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fe0745adab24c6a82877ca471e5c32f",
       "IPY_MODEL_8795346c4376409d9b5126236b8ecee6",
       "IPY_MODEL_15cd689a923f423087fb6945f7ba22d2"
      ],
      "layout": "IPY_MODEL_fa8540d5c08145e9b51cadbdc063b715"
     }
    },
    "8cd3e20119fe4fa2bde2606e310b507c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_896f716571074df9a0b4505da5a655ca",
      "max": 4014,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a93544672d484e55a177c8ddb4723c19",
      "value": 4014
     }
    },
    "8cd9b53d885246c7aa82a2ab03f6357b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8edc778a5055444ea63f48545e8089aa",
       "IPY_MODEL_abb09c1f043646b98a862ab673edb310",
       "IPY_MODEL_d98d3d5e8e4f4a96a99fba4abe773104"
      ],
      "layout": "IPY_MODEL_8658a29975974246af3df68e5e52198d"
     }
    },
    "8edc778a5055444ea63f48545e8089aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f598a0edd544bd4b39faac1de090b9d",
      "placeholder": "​",
      "style": "IPY_MODEL_7d5456b0ea6d4cbf95cd93e4db33485a",
      "value": "Downloading (…)5ca7e0d5/config.json: 100%"
     }
    },
    "92f959221add41669661b53b9ae66800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcdb66968f5e4dbe8c75641b4a65193c",
      "placeholder": "​",
      "style": "IPY_MODEL_0239d3187339442295198532950f9982",
      "value": "Batches: 100%"
     }
    },
    "93cc574a16bb4ee3aa37d7bb00807b95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0131633e5184851bf231fef45d8d942",
       "IPY_MODEL_2e7ff5c089c840c298df34c9085e7288",
       "IPY_MODEL_24f46851a05d4f0ba7cedf9fe6b854c1"
      ],
      "layout": "IPY_MODEL_10feddb6ec244d259c55144943e51987"
     }
    },
    "93fbf4a4c01847109b23b94c1561907e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97644a9c4fe148f0a1c62532d4a6fe5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e3677374c19471a84bb78c3dcddfca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbc3e919348e4292af76e770fdd6f06f",
      "placeholder": "​",
      "style": "IPY_MODEL_3477b89bd24d4f6f94a57741ce0f744b",
      "value": "Downloading (…)ce_transformers.json: 100%"
     }
    },
    "a4d6f98130a142588ce66414279823ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a728a2ef35d447f890603d9c9f79f15d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a93544672d484e55a177c8ddb4723c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa5ecefecef34cdba58ff4d4092ced56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abb09c1f043646b98a862ab673edb310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41869a9881ae42e7af2b7cef43a56f66",
      "max": 555,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_196c48db830f4a3bb2fcac35f673938d",
      "value": 555
     }
    },
    "ad6354fed78e4209a0eb7e5b0ef81fcc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0131633e5184851bf231fef45d8d942": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddd1482ada5843bcbee039f46430e3dc",
      "placeholder": "​",
      "style": "IPY_MODEL_28ee7786ee284adfb4b6cba3440a7a99",
      "value": "Downloading (…)ca7e0d5/modules.json: 100%"
     }
    },
    "b29f27e7283d468b9ae6f38dd29b0c6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5df03efae4f45dbb912cb8529154304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3709d5063ff242fda5150b1c8c6c4664",
      "placeholder": "​",
      "style": "IPY_MODEL_d20db335687b4289a9e0196710fa9f8e",
      "value": "Downloading (…)_Pooling/config.json: 100%"
     }
    },
    "b716708dc7474328992c5b26697aa4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a745e2bcba94554880c67aaa83c158b",
      "placeholder": "​",
      "style": "IPY_MODEL_54a15fdc18fb40029ebf9839225c40fc",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "b9700ea0ef3e4461a3c128d392838986": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb3f9420a43c490985220119e79e0dbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc31a80accf844c3a6a2c89b673cef9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ea10297a23249b7b1f4843e521e27de",
      "max": 122,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_deff838ffb814fbf87e9f202a8d94e00",
      "value": 122
     }
    },
    "bcdb66968f5e4dbe8c75641b4a65193c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdb19005a4e94f8a90c29260964304cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be20d532415c4f82b827ea0ea1cb59b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_770a8586da4844dc84366444a8f65637",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14257bc929f7424092463b34d0fc4a61",
      "value": 112
     }
    },
    "bf5553fb54764284906ee83adf69a558": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_92f959221add41669661b53b9ae66800",
       "IPY_MODEL_77f4a144135145dfa4fe20d478a0967f",
       "IPY_MODEL_4ffaad03b2f444fa930ef980a1c107f3"
      ],
      "layout": "IPY_MODEL_d45d032e07494aec84f4c19b351e8378"
     }
    },
    "bfd32f3063a34f4ea2859d8f40b86ca6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfe7598cb6584390981f157f4d7db1c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c010bca4b46c426cb4d75734ee4d213e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cab5e02948ca4895a51b911f6709b3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cad3b086340944a59375ece310d3a705": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d028083f47834b3491a025e1361757da",
       "IPY_MODEL_be20d532415c4f82b827ea0ea1cb59b8",
       "IPY_MODEL_780e3bdd45a5495592cc5fb44780f88c"
      ],
      "layout": "IPY_MODEL_4b15909c8b004279a389902115f89421"
     }
    },
    "d028083f47834b3491a025e1361757da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e797accb6ab54ee7b51bfdb525c4cccb",
      "placeholder": "​",
      "style": "IPY_MODEL_696574fd545c4b55be32ea07e094b7d6",
      "value": "Downloading (…)cial_tokens_map.json: 100%"
     }
    },
    "d0716fb9e26f4673b23c912b0720e2ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b5d0c5b41114272b23c5aaf6e2d5cf9",
      "placeholder": "​",
      "style": "IPY_MODEL_7ede626c3dde463b822a84e75a23face",
      "value": "Downloading (…)7e0d5/tokenizer.json: 100%"
     }
    },
    "d20db335687b4289a9e0196710fa9f8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d320f358e78c4d0a9bd0b5dc082a40b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d32bfdf2d9b2486089c2f6f81535d49e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d38bba863f9e404a9b865c0a2c03311c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37d30d9227094847b02c3f205904f1a8",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e1885f62864416aa8ac36c8fca91277",
      "value": 53
     }
    },
    "d45d032e07494aec84f4c19b351e8378": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d798a2cb981e4bea896447a54eec09cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da94a6e39c0f42c8809c4533d87dbbba",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1925808c688544bf9d071f588633b250",
      "value": 190
     }
    },
    "d80941e0bb7c406d8898300ba0267f36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d98d3d5e8e4f4a96a99fba4abe773104": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_734fc4060cc54b3eb64bfa32bd0975ff",
      "placeholder": "​",
      "style": "IPY_MODEL_6a2abb6db22348bca9b9da53b0320e0d",
      "value": " 555/555 [00:00&lt;00:00, 28.9kB/s]"
     }
    },
    "da94a6e39c0f42c8809c4533d87dbbba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddd1482ada5843bcbee039f46430e3dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deff838ffb814fbf87e9f202a8d94e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df7d9b5f58d6435187360a31e79b176a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4315e4e636c40f1a9205524642c0d7b",
      "max": 265486777,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d32bfdf2d9b2486089c2f6f81535d49e",
      "value": 265486777
     }
    },
    "e044383b011f4863a05fb95770b33781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0be8b9bc5d4c417ab28216732e69bd57",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f28ac8e1d9d648b2b285be59f4a353c1",
      "value": 231508
     }
    },
    "e797accb6ab54ee7b51bfdb525c4cccb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edb01c355cbc4e1eac31593ac2c8a555": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef36ada088a24063b6ed058c2faf133a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f28ac8e1d9d648b2b285be59f4a353c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4315e4e636c40f1a9205524642c0d7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f56e474aa3734728b699c6c2ed6c709e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa8540d5c08145e9b51cadbdc063b715": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb366cdce58c478f8a7e69a336d7d70e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbc3e919348e4292af76e770fdd6f06f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcdefdf5882c44b995cadce70b4d1211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ffcf6844c579427da16fae54fddf20dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
